{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DataPull DataPull is a self-service tool provided by HomeAway's Data Tools team to migrate data across heterogeneous datastores effortlessly. When deployed to Amazon AWS, DataPull spins up EMR Spark infrastructure, does the data movement and terminates the infrastructure once the job is complete; to minimize costs. Multiple data migrations can be done ither serially or in parallel within a DataPull job. There also exists built-in integration with Hashicorp Vault so that datastore credentials are never exposed. DataPull also has a built-in scheduler for daily/recurring jobs; or its REST API endpoints can be invoked by a third-party scheduler. Platforms supported DataPull supports the following datastores as sources and destinations. Platform Source Destination SQL Server \u2714 \u2714 Cassandra \u2714 \u2714 Mongodb \u2714 \u2714 S3 \u2714 \u2714 FileSystem \u2714 \u2714 SFTP \u2714 \u2714 Elasticsearch \u2714 \u2714 Kafka \u2714 X Neo4j X \u2714 MySql \u2714 \u2714 Postgres \u2714 \u2714 InfluxDB \u2714 \u2714 How to use DataPull Steps common to all environments (Dev/Test/Stage/Prod) Create a json file/string that has the source/s, destination/s and the portfolio information for tagging the ephemeral infrastructure needed to do the DataPull Here are some sample JSON files for some common use cases: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples Here is JSON specification document which has all the possible options supported by DataPull: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples/Input_Json_Specification.json Please provide an email address for the element \"useremailaddress\" in the JSON input. Once your DataPull completes, an email will be sent with all the migration details along with the source, destination details of the migrations, the time taken for the migrations to complete and the number of records processed, etc. For DataPull to get access to the source and destination data platforms, you need to either provide the login and password within the JSON (not recommended for Stage and Production environments); or provide the login and an IAM Role which is mapped to the credentials in Vault (you also need to add the \"awsenv\" and \"vaultenv\" elements to the source(s) and destination in the json input). Navigate to your swagger api URL and invoke this REST API with the json as input. More information about the Input JSON The input JSON mainly divided into Three sections. Basic Information about the Job Migrations Cluster The basic information section is there for allowing the users to put their email address, enabling the jobs to run in parallel or serial etc. Migrations section is an array and primarily covers source and the destination details of a migration. It can have multiple migration objects. And enabling the any options/features is very easy, we just have to add the element as specified in the Input_specifications document which is available in the resources folder. If you want to migrate only set of columns of the source data, DataPull supports this by using Mappings array inside a migration object and Please check the sample JSONs available for a example. Cluster section is mostly related to the infrastructure of the migration job which has details about the portfolio, product, Cron expression and environment in which the job will be running etc. The cluster will be spun up based on the awsenv(aws environment) element in this section among dev/test/stage/prod. This sample JSON input moves data from MySql to AWS S3 How does DataPull work? Spark is the main engine which drives the DataPull. So technically it will run on any spark environment. But we are biased to Amazon's Elastic Map Reduce(EMR) to have minimum dependencies(AWS versus AWS+Qubole) and our app does use EMR to spin up the spark clusters. But after all as it is a spark application it will run on any spark cluster whether it is EMR or Qubole or any other spark clusters. DataPull expects the Input JSON as an argument and irrespective of any spark environment we have to pass the Input JSON as an argument to make the DataPull work. How do I do Delta Moves for DataPull? Here is an example { \"useremailaddress\": \"your_id@expediagroup.com\", \"migrations\": [ { \"sources\": [{ \"platform\": \"cassandra\", \"cluster\": \"cassandracluster\", \"keyspace\": \"terms_conditions\", \"table\": \"your_table\", \"login\": \"your_appuser\", \"awsenv\": \"prod\", \"vaultenv\": \"prod\", \"alias\":\"source\" }, { \"platform\": \"s3\", \"s3path\": \"bucketpath\", \"fileformat\": \"json\", \"alias\":\"destination\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucketpath\", \"fileformat\": \"json\" }, \"sql\":{ \"query\":\"select source.* from source LEFT JOIN destination on source.id= destination.id where destination.id=null\" } } ], \"cluster\": { \"pipelinename\": \"yourpipeline\", \"awsenv\": \"prod\", \"portfolio\": \"payment-services\", \"product\": \"payment-onboarding\", \"ec2instanceprofile\": \"instanceprofile\", \"cronexpression\": \"0/15 * * * *\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\": \"someuuid\" } } Run your DataPull on a spark cluster If you are starting from scratch... Prepare the Input JSON as explained in the above section with all the required information. Then please go to swagger API URL and submit the JSON in the inputJson field provided and click Try it out! For Elastic Search In the field of mappingid it is recommended to use _id or id as fieldnames as they are a unique keyword in Elastic Search. How to use delta DataPulls? So lets say you did a DataPull, however in the second datapull you do not want to repopulate everything but only need the deltas of new records. Here is what is needed. Pre-req: CDC needs to enabled on the database Setup CDC for your specified tables. It creates a watermarking table which essentially allows you to calculate the deltas. Next is you create your json but with Pre-migrate and Post migrate steps. In your pre-migrate step you populate your watermarking table with defaults and you update the watermark_from with watermark to field. And as part of post migration field you reset the watermark_to field with now date so you are ready for the future datapull Once this is complete you can revert back to the original of how to use DataPull again. How to schedule a DataPull ? Please add an element cronexpression in the cluster section of the Input Json. For example, \"cronexpression\": \"0 21 * * *\" executes DataPull every day at 9 PM UTC. Contributors to DataPull The current list of contributors are tracked by Github at https://github.com/homeaway/datapull/graphs/contributors . Prior to being opensourced, DataPull was an innersourced project at Vrbo, that evolved with contributions from Arturo Artigas Nirav Shah Ranjith Peddi Rohith Mark Varghese Sandeep Nautiyal Satish Behra Selvanathan Ragunathan Srinivas Rao Gajjala Virendra Chaudhary","title":"Home"},{"location":"#datapull","text":"DataPull is a self-service tool provided by HomeAway's Data Tools team to migrate data across heterogeneous datastores effortlessly. When deployed to Amazon AWS, DataPull spins up EMR Spark infrastructure, does the data movement and terminates the infrastructure once the job is complete; to minimize costs. Multiple data migrations can be done ither serially or in parallel within a DataPull job. There also exists built-in integration with Hashicorp Vault so that datastore credentials are never exposed. DataPull also has a built-in scheduler for daily/recurring jobs; or its REST API endpoints can be invoked by a third-party scheduler.","title":"DataPull"},{"location":"#platforms-supported","text":"DataPull supports the following datastores as sources and destinations. Platform Source Destination SQL Server \u2714 \u2714 Cassandra \u2714 \u2714 Mongodb \u2714 \u2714 S3 \u2714 \u2714 FileSystem \u2714 \u2714 SFTP \u2714 \u2714 Elasticsearch \u2714 \u2714 Kafka \u2714 X Neo4j X \u2714 MySql \u2714 \u2714 Postgres \u2714 \u2714 InfluxDB \u2714 \u2714","title":"Platforms supported"},{"location":"#how-to-use-datapull","text":"","title":"How to use DataPull"},{"location":"#steps-common-to-all-environments-devteststageprod","text":"Create a json file/string that has the source/s, destination/s and the portfolio information for tagging the ephemeral infrastructure needed to do the DataPull Here are some sample JSON files for some common use cases: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples Here is JSON specification document which has all the possible options supported by DataPull: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples/Input_Json_Specification.json Please provide an email address for the element \"useremailaddress\" in the JSON input. Once your DataPull completes, an email will be sent with all the migration details along with the source, destination details of the migrations, the time taken for the migrations to complete and the number of records processed, etc. For DataPull to get access to the source and destination data platforms, you need to either provide the login and password within the JSON (not recommended for Stage and Production environments); or provide the login and an IAM Role which is mapped to the credentials in Vault (you also need to add the \"awsenv\" and \"vaultenv\" elements to the source(s) and destination in the json input). Navigate to your swagger api URL and invoke this REST API with the json as input.","title":"Steps common to all environments (Dev/Test/Stage/Prod)"},{"location":"#more-information-about-the-input-json","text":"The input JSON mainly divided into Three sections. Basic Information about the Job Migrations Cluster The basic information section is there for allowing the users to put their email address, enabling the jobs to run in parallel or serial etc. Migrations section is an array and primarily covers source and the destination details of a migration. It can have multiple migration objects. And enabling the any options/features is very easy, we just have to add the element as specified in the Input_specifications document which is available in the resources folder. If you want to migrate only set of columns of the source data, DataPull supports this by using Mappings array inside a migration object and Please check the sample JSONs available for a example. Cluster section is mostly related to the infrastructure of the migration job which has details about the portfolio, product, Cron expression and environment in which the job will be running etc. The cluster will be spun up based on the awsenv(aws environment) element in this section among dev/test/stage/prod. This sample JSON input moves data from MySql to AWS S3","title":"More information about the Input JSON"},{"location":"#how-does-datapull-work","text":"Spark is the main engine which drives the DataPull. So technically it will run on any spark environment. But we are biased to Amazon's Elastic Map Reduce(EMR) to have minimum dependencies(AWS versus AWS+Qubole) and our app does use EMR to spin up the spark clusters. But after all as it is a spark application it will run on any spark cluster whether it is EMR or Qubole or any other spark clusters. DataPull expects the Input JSON as an argument and irrespective of any spark environment we have to pass the Input JSON as an argument to make the DataPull work.","title":"How does DataPull work?"},{"location":"#how-do-i-do-delta-moves-for-datapull","text":"Here is an example { \"useremailaddress\": \"your_id@expediagroup.com\", \"migrations\": [ { \"sources\": [{ \"platform\": \"cassandra\", \"cluster\": \"cassandracluster\", \"keyspace\": \"terms_conditions\", \"table\": \"your_table\", \"login\": \"your_appuser\", \"awsenv\": \"prod\", \"vaultenv\": \"prod\", \"alias\":\"source\" }, { \"platform\": \"s3\", \"s3path\": \"bucketpath\", \"fileformat\": \"json\", \"alias\":\"destination\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucketpath\", \"fileformat\": \"json\" }, \"sql\":{ \"query\":\"select source.* from source LEFT JOIN destination on source.id= destination.id where destination.id=null\" } } ], \"cluster\": { \"pipelinename\": \"yourpipeline\", \"awsenv\": \"prod\", \"portfolio\": \"payment-services\", \"product\": \"payment-onboarding\", \"ec2instanceprofile\": \"instanceprofile\", \"cronexpression\": \"0/15 * * * *\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\": \"someuuid\" } }","title":"How do I do Delta Moves for DataPull?"},{"location":"#run-your-datapull-on-a-spark-cluster","text":"","title":"Run your DataPull on a spark cluster"},{"location":"#if-you-are-starting-from-scratch","text":"Prepare the Input JSON as explained in the above section with all the required information. Then please go to swagger API URL and submit the JSON in the inputJson field provided and click Try it out!","title":"If you are starting from scratch..."},{"location":"#for-elastic-search-in-the-field-of-mappingid-it-is-recommended-to-use-_id-or-id-as-fieldnames-as-they-are-a-unique-keyword-in-elastic-search","text":"","title":"For Elastic Search In the field of mappingid it is recommended to use _id or id as fieldnames as they are a unique keyword in Elastic Search."},{"location":"#how-to-use-delta-datapulls","text":"So lets say you did a DataPull, however in the second datapull you do not want to repopulate everything but only need the deltas of new records. Here is what is needed. Pre-req: CDC needs to enabled on the database Setup CDC for your specified tables. It creates a watermarking table which essentially allows you to calculate the deltas. Next is you create your json but with Pre-migrate and Post migrate steps. In your pre-migrate step you populate your watermarking table with defaults and you update the watermark_from with watermark to field. And as part of post migration field you reset the watermark_to field with now date so you are ready for the future datapull Once this is complete you can revert back to the original of how to use DataPull again.","title":"How to use delta DataPulls?"},{"location":"#how-to-schedule-a-datapull","text":"Please add an element cronexpression in the cluster section of the Input Json. For example, \"cronexpression\": \"0 21 * * *\" executes DataPull every day at 9 PM UTC.","title":"How to schedule a DataPull ?"},{"location":"#contributors-to-datapull","text":"The current list of contributors are tracked by Github at https://github.com/homeaway/datapull/graphs/contributors . Prior to being opensourced, DataPull was an innersourced project at Vrbo, that evolved with contributions from Arturo Artigas Nirav Shah Ranjith Peddi Rohith Mark Varghese Sandeep Nautiyal Satish Behra Selvanathan Ragunathan Srinivas Rao Gajjala Virendra Chaudhary","title":"Contributors to DataPull"},{"location":"architecture/","text":"","title":"Architecture"},{"location":"aws_account_setup/","text":"Setup VPC etc. in AWS Account for DataPull install The instructions for deploying DataPull on AWS Fargate and AWS EMR assume that you already have available an S3 bucket, a VPC, subnets, etc. If you do not have these, or if you want to install DataPull in a new VPC dedicated to DataPull, please follow the following instructions. It is generally recommended to use your existing VPC, subnets, etc. since they are most likely already setup to access the data you want DatPull to work on, have access to other services like S3, etc. When creating a VPC for DataPull, you need at least two subnets in different availability zones, since AWS Application Load Balancer requires a minimum of two availability zones. You can create a VPC with two public subnets (approach modeled on https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario1.html) or a VPC with two private subnets and a public subnet (modeled on https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html) VPC with two public subnets create VPC datatools with 2 IPv4 ranges 10.0.0.0/24 this creates a security group named default for this VPC this also creates a main route table with no name add a rule to security group default : to allow inbound traffic to tcp port 22 (SSH) from your client This assumes that you will have a Bastion node or equivalent to access DataPull's API and Spark UI. If you trust your client network's public IP to be static or not be spoofed, you can allow all inbound traffic from your client, and thus obviate the need for a Bastion node or equivalent this also creates a main route table with no name create two subnets in the VPC, each one in a different availability zone in your region. subnet datatools-external-1 with IPv4 range 10.0.0.0/25 subnet datatools-external-2 with IPv4 range 10.0.0.128/25 create internet gateway datatools for vpc datatools create route for destination 0.0.0.0/0 to internet gateway datatools for default the route table for the VPC VPC with two private subnets and a public subnet create VPC datatools with 2 IPv4 ranges 10.0.0.0/24 , 10.1.0.0/24 this creates a security group named default for this VPC this also creates a main route table with no name add a rule to security group default : to allow inbound traffic to tcp port 22 (SSH) from your client network (or from your public IP as a last resort) create three subnets in the VPC, each one in a different availability zone in your region. subnet datatools-internal-1 with IPv4 range 10.0.0.0/25 subnet datatools-internal-2 with IPv4 range 10.0.0.128/25 subnet datatools-external-1 with IPv4 range 10.1.0.0/24 create internet gateway datatools for vpc datatools create route table datatools-external in vpc datatools create route for detination 0.0.0.0/0 to internet gateway datatools for route table datatools-external associate subnet datatools-external-1 to route table datatools-external subnets datatools-internal-[1-2] will remain associated to the main/default route table (not route table datatools-external ) for the VPC datatools create NAT gateway in the subnet datatools-external-1 and with a new/existing elastic IP. add an entry to the default route table with destination as 0.0.0.0/0 and Target as the NAT gateway Additional Steps (common to both VPC types above) create S3 bucket datatools-datapull with SSE-S3 encryption in the same region as the VPC create gateway endpoint for S3, associated with VPC datatools and the default route table (not route table datatools-external ). The policy should be Full Access create a bastion host by following https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/ with existing keypair, else no SSH possible security groups default (that allows SSH connections from your client network) and ElasticMapReduce-Master-Private (to allow bastion node to connect to EMR master) For simplicity (at the cost of auditing and security features like 2FA), you can spin up a t2.micro EC2 instance with AWS Linux 2 AMI as an alternative to a bastion host; it will allow ssh tunneling. If you are using a VPC with only public subnets, and if you trust your client network's public IP to be static or not be spoofed, you can allow all inbound traffic from your client, and thus obviate the need for a Bastion node or equivalent","title":"Setup VPC etc. in AWS Account for DataPull install"},{"location":"aws_account_setup/#setup-vpc-etc-in-aws-account-for-datapull-install","text":"The instructions for deploying DataPull on AWS Fargate and AWS EMR assume that you already have available an S3 bucket, a VPC, subnets, etc. If you do not have these, or if you want to install DataPull in a new VPC dedicated to DataPull, please follow the following instructions. It is generally recommended to use your existing VPC, subnets, etc. since they are most likely already setup to access the data you want DatPull to work on, have access to other services like S3, etc. When creating a VPC for DataPull, you need at least two subnets in different availability zones, since AWS Application Load Balancer requires a minimum of two availability zones. You can create a VPC with two public subnets (approach modeled on https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario1.html) or a VPC with two private subnets and a public subnet (modeled on https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html)","title":"Setup VPC etc. in AWS Account for DataPull install"},{"location":"aws_account_setup/#vpc-with-two-public-subnets","text":"create VPC datatools with 2 IPv4 ranges 10.0.0.0/24 this creates a security group named default for this VPC this also creates a main route table with no name add a rule to security group default : to allow inbound traffic to tcp port 22 (SSH) from your client This assumes that you will have a Bastion node or equivalent to access DataPull's API and Spark UI. If you trust your client network's public IP to be static or not be spoofed, you can allow all inbound traffic from your client, and thus obviate the need for a Bastion node or equivalent this also creates a main route table with no name create two subnets in the VPC, each one in a different availability zone in your region. subnet datatools-external-1 with IPv4 range 10.0.0.0/25 subnet datatools-external-2 with IPv4 range 10.0.0.128/25 create internet gateway datatools for vpc datatools create route for destination 0.0.0.0/0 to internet gateway datatools for default the route table for the VPC","title":"VPC with two public subnets"},{"location":"aws_account_setup/#vpc-with-two-private-subnets-and-a-public-subnet","text":"create VPC datatools with 2 IPv4 ranges 10.0.0.0/24 , 10.1.0.0/24 this creates a security group named default for this VPC this also creates a main route table with no name add a rule to security group default : to allow inbound traffic to tcp port 22 (SSH) from your client network (or from your public IP as a last resort) create three subnets in the VPC, each one in a different availability zone in your region. subnet datatools-internal-1 with IPv4 range 10.0.0.0/25 subnet datatools-internal-2 with IPv4 range 10.0.0.128/25 subnet datatools-external-1 with IPv4 range 10.1.0.0/24 create internet gateway datatools for vpc datatools create route table datatools-external in vpc datatools create route for detination 0.0.0.0/0 to internet gateway datatools for route table datatools-external associate subnet datatools-external-1 to route table datatools-external subnets datatools-internal-[1-2] will remain associated to the main/default route table (not route table datatools-external ) for the VPC datatools create NAT gateway in the subnet datatools-external-1 and with a new/existing elastic IP. add an entry to the default route table with destination as 0.0.0.0/0 and Target as the NAT gateway","title":"VPC with two private subnets and a public subnet"},{"location":"aws_account_setup/#additional-steps-common-to-both-vpc-types-above","text":"create S3 bucket datatools-datapull with SSE-S3 encryption in the same region as the VPC create gateway endpoint for S3, associated with VPC datatools and the default route table (not route table datatools-external ). The policy should be Full Access create a bastion host by following https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/ with existing keypair, else no SSH possible security groups default (that allows SSH connections from your client network) and ElasticMapReduce-Master-Private (to allow bastion node to connect to EMR master) For simplicity (at the cost of auditing and security features like 2FA), you can spin up a t2.micro EC2 instance with AWS Linux 2 AMI as an alternative to a bastion host; it will allow ssh tunneling. If you are using a VPC with only public subnets, and if you trust your client network's public IP to be static or not be spoofed, you can allow all inbound traffic from your client, and thus obviate the need for a Bastion node or equivalent","title":"Additional Steps (common to both VPC types above)"},{"location":"emr_runbook/","text":"Runbook for using DataPull deployed on AWS EMR 1. Prepare the Pipeline JSON 1.1 Here's a sample JSON that joins Property information from SQL Server with units in cassandra and moves to s3. Please copy this to your favorite text editor { \"useremailaddress\": \"<EMAIL_ADDRESS>\", \"migrations\": [ { \"sources\": [ { \"platform\": \"mssql\", \"server\": \"server_name\", \"database\": \"db_name\", \"table\": \"properties_table\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"properties\" }, { \"platform\": \"cassandra\", \"cluster\": \"cassandra_server_name\", \"keyspace\": \"keyspace_name\", \"table\": \"units\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"units\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucket_name/path/\", \"fileformat\": \"csv\" }, \"sql\": { \"query\": \"select properties.pro_id, properties.pro_city, units.unituuid from properties JOIN units ON properties.pro_id = units.propertyid limit 100\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } } 1.2 In the JSON in your text editor, please replace <EMAIL_ADDRESS> with your email address. database and cluster details with your database and cluster details. 2. Submit the JSON to the API 2.1 Copy the modified JSON from your text editor, and paste it into the swagger API inputJson. Swagger API can be accessed through URL - http://IP-Address:8080/swagger-ui.html#!/data45pull45request45handler/startDatapull . If api is deployed in ECS FARGATE, replace the IP-Address with IP address of task of FARGATE app. If it is deployed in local, replace IP-Address with localhost. 2.2 Click the \"Try it out!\" button to queue the data pull. 3. Check status of the job We can check the status of the job in spark UI. Spark UI runs on the master node of the EMR cluster. URL of the Spark UI will be http://master-node-IP:8088/cluster So, to find the master node IP address ,login into your AWS console, navigate to EMR service and find your cluster. If pipelinename is datapull and awsenv is dev, the cluster name will be dev-emr-datapull-pipeline. Find the IP address of the master node from hardware tab. 4. Get the email confirmation Wait for a few minutes, and check your email to get a confirmation when your data pull job is complete. 5. Check S3 for your data Log into your AWS console. Navigate to your S3 bucket. Check the data under s3 path provided in the input json.","title":"Running DataPull on AWS EMR"},{"location":"emr_runbook/#runbook-for-using-datapull-deployed-on-aws-emr","text":"","title":"Runbook for using DataPull deployed on AWS EMR"},{"location":"emr_runbook/#1-prepare-the-pipeline-json","text":"1.1 Here's a sample JSON that joins Property information from SQL Server with units in cassandra and moves to s3. Please copy this to your favorite text editor { \"useremailaddress\": \"<EMAIL_ADDRESS>\", \"migrations\": [ { \"sources\": [ { \"platform\": \"mssql\", \"server\": \"server_name\", \"database\": \"db_name\", \"table\": \"properties_table\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"properties\" }, { \"platform\": \"cassandra\", \"cluster\": \"cassandra_server_name\", \"keyspace\": \"keyspace_name\", \"table\": \"units\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"units\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucket_name/path/\", \"fileformat\": \"csv\" }, \"sql\": { \"query\": \"select properties.pro_id, properties.pro_city, units.unituuid from properties JOIN units ON properties.pro_id = units.propertyid limit 100\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } } 1.2 In the JSON in your text editor, please replace <EMAIL_ADDRESS> with your email address. database and cluster details with your database and cluster details.","title":"1. Prepare the Pipeline JSON"},{"location":"emr_runbook/#2-submit-the-json-to-the-api","text":"2.1 Copy the modified JSON from your text editor, and paste it into the swagger API inputJson. Swagger API can be accessed through URL - http://IP-Address:8080/swagger-ui.html#!/data45pull45request45handler/startDatapull . If api is deployed in ECS FARGATE, replace the IP-Address with IP address of task of FARGATE app. If it is deployed in local, replace IP-Address with localhost. 2.2 Click the \"Try it out!\" button to queue the data pull.","title":"2. Submit the JSON to the API"},{"location":"emr_runbook/#3-check-status-of-the-job","text":"We can check the status of the job in spark UI. Spark UI runs on the master node of the EMR cluster. URL of the Spark UI will be http://master-node-IP:8088/cluster So, to find the master node IP address ,login into your AWS console, navigate to EMR service and find your cluster. If pipelinename is datapull and awsenv is dev, the cluster name will be dev-emr-datapull-pipeline. Find the IP address of the master node from hardware tab.","title":"3. Check status of the job"},{"location":"emr_runbook/#4-get-the-email-confirmation","text":"Wait for a few minutes, and check your email to get a confirmation when your data pull job is complete.","title":"4. Get the email confirmation"},{"location":"emr_runbook/#5-check-s3-for-your-data","text":"Log into your AWS console. Navigate to your S3 bucket. Check the data under s3 path provided in the input json.","title":"5. Check S3 for your data"},{"location":"faq/","text":"FAQs Can I use datapull to bulk delete rows in my Cassandra database? The SQL used for migrating Cassandra to Cassandra cannot be used to issue DELETE s, but it can update the TTL on rows using a spark option. To run a bulk delete, just set both the source and destination to the same table, select only the rows that you want to delete, and set a small TTL on them using a nonzero value in the spark.cassandra.output.ttl field in your sparkoptions block. (This is technically only an update, but it will have the desired effect of deleting the selected rows when the TTL runs out.) Example: In a table with columns foo , bar , and baz , the following block in your input JSON could be used to delete all rows where baz is set to true . { \"migrations\": [ { \"source\": { ... \"table\": \"my_foo_table\", \"alias\": \"source_table\" }, \"sql\":{ \"query\": \"SELECT foo, bar, baz FROM source_table WHERE baz = true\" }, \"destination\": { ... \"table\": \"my_foo_table\", \"sparkoptions\" : { \"spark.cassandra.output.ttl\" : \"1\" } } } ] } !!! note For Elastic Search as a platform, it is recommended not to use _id or id as field names of mappingid as they are unique keyword in Elastic Search. How can I submit a DataPull job which should run exactly once? To make the DataPull job to run only once, we can remove the cronexpression parameter from the json, then it will be executed only once. How to replace or upsert documents in MongoDB using DataPull? We can use replace documents by setting up the option replacedocuments to true (By default it is true) which will replace the whole document with the recent one which is having the same _id. If we want to upsert new columns to the existing documents we can the replacedocuments to false and we have to explicitly select use the same _id along with the new columns and in case if we don't have the same _id on the source side then we can read destination mongo collection as one of the source and joining that with the other source/s then pushing it to the destination will upsert the documents existing. How does DataPull behave when reading data from MongoDB collection which is having more than one schema? DataPull uses dataframes to move data from source/s to destination. Dataframes are schema bound i.e it expects a single schema across the whole dataset. To find the schema of a Mongo Collection, DataPull will sample a few documents in the Collection (the sample size is configurable). To conclude Datapull can't reliably migrate a collection/dataset which has multiple schemas in it. How do I convert an Array of Doubles/Strings to a string using Spark SQL? For example - \"categories\":[\"geoAdmin:continent\",\"meta:highLevelRegion\"] then we can use CONCAT('[',concat_ws(',',categories),']') as categories . How do I convert Array of Jsons to a String? For example - \"localizedNames\":[{\"lcid\":1025,\"value\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\",\"extendedValue\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\"},{\"lcid\":1028,\"value\":\"\u5357\u6975\u6d32\",\"extendedValue\":\"\u5357\u6975\u6d32\"}] then we can use to_json(localizedNames) as localizedNames How do I convert String UUID to Type4 UUID? For moving data to MongoDB with custom _id or any UUID's as UUID than as a String, we wrote a custom function. we can use uuidToBinary(uuid_colum) as _id in the sql column of the json. and can use binaryToUUID(_id) as column_name to do the vice versa. For any Spark SQL functions please refer to https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/functions.html and If you don't find any, feel free to let us know we are happy to write the custom function or you can contribute back to the tool. How to I create a complex document structure with nested arrays and subdocuments, using Spark SQL? Let's assume you want to create an array within a document within an array within a document, like this ... { \"unitUuid\": \"00000000-0000-0000-0000-000000000000\", \"unitInfo\": [ { \"UnitID\": \"3000523\", \"Y\": [ \"204785\" ] } ] } Here's the Spark SQL statement that will produce the above document. select UA.unitUuid , UA.cibEnabled , ( select collect_set ( named_struct( 'UnitID', U.unitId , 'Y', ( select collect_set(P.listingNumber) from P where U.propertyEntityId = P._id ) ) ) from U where UA.unitEntityId = U._id ) as unitInfo from UA Common errors and their fixes EMR Pipeline errors Symptom You submit a DataPull job either through the UI or through the REST API endpoint and the EMR cluster isn't created. Fixes Check if IAM role or secret and access key given to API service is having permission to create EMR cluster. If this is a scheduled job (i.e you have specified a cron expression in your JSON input) the pipeline will not start building until the scheduled time (which is in UTC) becomes current. If this is an ad-hoc job (i.e. you haven't specified a cron expression in your JSON input) the system automatically schedules the pipeline to run approximately 5 minutes from the time you submitted the JSON through the UI/REST api endpoint. Symptom You are unable to move data from an S3 bucket in Dev environment to an InfluxDB cluster in Production environment Fixes This is a known limitation that affects InfluxDB alone; and there is an easy workaround - Do a DataPull from the S3 bucket in the Dev environment to an S3 bucket in the Production environment; using a Spark cluster in the Production environment. - You will need to provide the AWS access key and secret key for the Dev environment, in the input json. - You need not provide the AWS access key and secret key for the Production environment, in the input json since the IAM role that the Spark cluster runs on would/should usually have access to the S3 buckets in the same environment. - Do a DataPull from the S3 bucket in the Production environment to the InfluxDB cluster in the production environment; using a Spark cluster in the Production environment. !!! note \"Please note that\" - this limitation does not affect any other source-destination pair as of 2019-01-08 i.e. you can move data from a dev S3 bucket to a production MongoDB cluster; you can move data from a dev Cassandra cluster to a production InfluxDb cluster etc. - if Production Isolation is fully enforced i.e. once there is no network access between the production and non-production environments, no tool including DataPull will be able to move data between production and non-Production environments without approval from the Security team and an exemption from the Network team. Vault errors Symptom You get the following error in the email report java.io.IOException: Server returned HTTP response code: 403 for URL: https://vault-url/clustername/login at Fixes The issue here is that Vault was unable to find the password for your clustername and loginname. Please check the following The clustername and login name are case-sensitive. The clustername in Vault does not match the name you provided in Vault. Getting error when reading from a cassandra table from the local environment? Error: INFO FileFormatWriter: Job null committed. Exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError: org/apache/commons/configuration/ConfigurationException at org.apache.spark.sql.cassandra.DefaultSource$.<init>(DefaultSource.scala:135) at org.apache.spark.sql.cassandra.DefaultSource$.<clinit>(DefaultSource.scala) Fixes Please add the below dependency to the core pom and re run the job. <dependency> <groupId>commons-configuration</groupId> <artifactId>commons-configuration</artifactId> <version>1.10</version> </dependency>","title":"FAQs"},{"location":"faq/#faqs","text":"","title":"FAQs"},{"location":"faq/#can-i-use-datapull-to-bulk-delete-rows-in-my-cassandra-database","text":"The SQL used for migrating Cassandra to Cassandra cannot be used to issue DELETE s, but it can update the TTL on rows using a spark option. To run a bulk delete, just set both the source and destination to the same table, select only the rows that you want to delete, and set a small TTL on them using a nonzero value in the spark.cassandra.output.ttl field in your sparkoptions block. (This is technically only an update, but it will have the desired effect of deleting the selected rows when the TTL runs out.) Example: In a table with columns foo , bar , and baz , the following block in your input JSON could be used to delete all rows where baz is set to true . { \"migrations\": [ { \"source\": { ... \"table\": \"my_foo_table\", \"alias\": \"source_table\" }, \"sql\":{ \"query\": \"SELECT foo, bar, baz FROM source_table WHERE baz = true\" }, \"destination\": { ... \"table\": \"my_foo_table\", \"sparkoptions\" : { \"spark.cassandra.output.ttl\" : \"1\" } } } ] } !!! note For Elastic Search as a platform, it is recommended not to use _id or id as field names of mappingid as they are unique keyword in Elastic Search.","title":"Can I use datapull to bulk delete rows in my Cassandra database?"},{"location":"faq/#how-can-i-submit-a-datapull-job-which-should-run-exactly-once","text":"To make the DataPull job to run only once, we can remove the cronexpression parameter from the json, then it will be executed only once.","title":"How can I submit a DataPull job which should run exactly once?"},{"location":"faq/#how-to-replace-or-upsert-documents-in-mongodb-using-datapull","text":"We can use replace documents by setting up the option replacedocuments to true (By default it is true) which will replace the whole document with the recent one which is having the same _id. If we want to upsert new columns to the existing documents we can the replacedocuments to false and we have to explicitly select use the same _id along with the new columns and in case if we don't have the same _id on the source side then we can read destination mongo collection as one of the source and joining that with the other source/s then pushing it to the destination will upsert the documents existing.","title":"How to replace or upsert documents in MongoDB using DataPull?"},{"location":"faq/#how-does-datapull-behave-when-reading-data-from-mongodb-collection-which-is-having-more-than-one-schema","text":"DataPull uses dataframes to move data from source/s to destination. Dataframes are schema bound i.e it expects a single schema across the whole dataset. To find the schema of a Mongo Collection, DataPull will sample a few documents in the Collection (the sample size is configurable). To conclude Datapull can't reliably migrate a collection/dataset which has multiple schemas in it.","title":"How does DataPull behave when reading data from MongoDB collection which is having more than one schema?"},{"location":"faq/#how-do-i-convert-an-array-of-doublesstrings-to-a-string-using-spark-sql","text":"For example - \"categories\":[\"geoAdmin:continent\",\"meta:highLevelRegion\"] then we can use CONCAT('[',concat_ws(',',categories),']') as categories .","title":"How do I convert an Array of Doubles/Strings to a string  using Spark SQL?"},{"location":"faq/#how-do-i-convert-array-of-jsons-to-a-string","text":"For example - \"localizedNames\":[{\"lcid\":1025,\"value\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\",\"extendedValue\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\"},{\"lcid\":1028,\"value\":\"\u5357\u6975\u6d32\",\"extendedValue\":\"\u5357\u6975\u6d32\"}] then we can use to_json(localizedNames) as localizedNames","title":"How do I convert Array of Jsons to a String?"},{"location":"faq/#how-do-i-convert-string-uuid-to-type4-uuid","text":"For moving data to MongoDB with custom _id or any UUID's as UUID than as a String, we wrote a custom function. we can use uuidToBinary(uuid_colum) as _id in the sql column of the json. and can use binaryToUUID(_id) as column_name to do the vice versa. For any Spark SQL functions please refer to https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/functions.html and If you don't find any, feel free to let us know we are happy to write the custom function or you can contribute back to the tool.","title":"How do I convert String UUID to Type4 UUID?"},{"location":"faq/#how-to-i-create-a-complex-document-structure-with-nested-arrays-and-subdocuments-using-spark-sql","text":"Let's assume you want to create an array within a document within an array within a document, like this ... { \"unitUuid\": \"00000000-0000-0000-0000-000000000000\", \"unitInfo\": [ { \"UnitID\": \"3000523\", \"Y\": [ \"204785\" ] } ] } Here's the Spark SQL statement that will produce the above document. select UA.unitUuid , UA.cibEnabled , ( select collect_set ( named_struct( 'UnitID', U.unitId , 'Y', ( select collect_set(P.listingNumber) from P where U.propertyEntityId = P._id ) ) ) from U where UA.unitEntityId = U._id ) as unitInfo from UA","title":"How to I create a complex document structure with nested arrays and subdocuments, using Spark SQL?"},{"location":"faq/#common-errors-and-their-fixes","text":"","title":"Common errors and their fixes"},{"location":"faq/#emr-pipeline-errors","text":"","title":"EMR Pipeline errors"},{"location":"faq/#symptom","text":"You submit a DataPull job either through the UI or through the REST API endpoint and the EMR cluster isn't created.","title":"Symptom"},{"location":"faq/#fixes","text":"Check if IAM role or secret and access key given to API service is having permission to create EMR cluster. If this is a scheduled job (i.e you have specified a cron expression in your JSON input) the pipeline will not start building until the scheduled time (which is in UTC) becomes current. If this is an ad-hoc job (i.e. you haven't specified a cron expression in your JSON input) the system automatically schedules the pipeline to run approximately 5 minutes from the time you submitted the JSON through the UI/REST api endpoint.","title":"Fixes"},{"location":"faq/#symptom_1","text":"You are unable to move data from an S3 bucket in Dev environment to an InfluxDB cluster in Production environment","title":"Symptom"},{"location":"faq/#fixes_1","text":"This is a known limitation that affects InfluxDB alone; and there is an easy workaround - Do a DataPull from the S3 bucket in the Dev environment to an S3 bucket in the Production environment; using a Spark cluster in the Production environment. - You will need to provide the AWS access key and secret key for the Dev environment, in the input json. - You need not provide the AWS access key and secret key for the Production environment, in the input json since the IAM role that the Spark cluster runs on would/should usually have access to the S3 buckets in the same environment. - Do a DataPull from the S3 bucket in the Production environment to the InfluxDB cluster in the production environment; using a Spark cluster in the Production environment. !!! note \"Please note that\" - this limitation does not affect any other source-destination pair as of 2019-01-08 i.e. you can move data from a dev S3 bucket to a production MongoDB cluster; you can move data from a dev Cassandra cluster to a production InfluxDb cluster etc. - if Production Isolation is fully enforced i.e. once there is no network access between the production and non-production environments, no tool including DataPull will be able to move data between production and non-Production environments without approval from the Security team and an exemption from the Network team.","title":"Fixes"},{"location":"faq/#vault-errors","text":"","title":"Vault errors"},{"location":"faq/#symptom_2","text":"You get the following error in the email report java.io.IOException: Server returned HTTP response code: 403 for URL: https://vault-url/clustername/login at","title":"Symptom"},{"location":"faq/#fixes_2","text":"The issue here is that Vault was unable to find the password for your clustername and loginname. Please check the following The clustername and login name are case-sensitive. The clustername in Vault does not match the name you provided in Vault.","title":"Fixes"},{"location":"faq/#getting-error-when-reading-from-a-cassandra-table-from-the-local-environment","text":"Error: INFO FileFormatWriter: Job null committed. Exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError: org/apache/commons/configuration/ConfigurationException at org.apache.spark.sql.cassandra.DefaultSource$.<init>(DefaultSource.scala:135) at org.apache.spark.sql.cassandra.DefaultSource$.<clinit>(DefaultSource.scala)","title":"Getting error when reading from a cassandra table from the local environment?"},{"location":"faq/#fixes_3","text":"Please add the below dependency to the core pom and re run the job. <dependency> <groupId>commons-configuration</groupId> <artifactId>commons-configuration</artifactId> <version>1.10</version> </dependency>","title":"Fixes"},{"location":"install_on_aws/","text":"Deploying DataPull on AWS Fargate/ECS and AWS EMR This document helps you install DataPull on an Amazon AWS account, and run your first DataPull job of converting CSV data in AWS S3 to JSON data. In a nutshell, deploying DataPull to an AWS Account creates three IAM Roles datapull_task_role , datapull_task_execution_role for running the DataPull REST API on AWS Fargate emr_ec2_datapull_role for running ephemeral AWS EMR clusters creates an IAM User datapull_user temporarily for the purpose of installing the following DataPull components an AWS Fargate service datapull-web-api with an associated image in AWS ECR, and AWS Application Load Balancer (ALB) an AWS CloudWatch Log Group datapull_cloudwatch_log_group and associated log stream stores DataPull JAR for EMR, job history, EMR logs in an existing AWS S3 bucket Note: <xyz> denotes a variable/attribute named xyz in the DataPull configuration file Pre-install steps Clone/download the master branch of this repo for fargate based install or master_ecs_ec2 branch if you want to do ecs based install. Have available, the AWS Profile <aws_admin_profile> of an IAM user/role that can create IAM users and IAM roles in your AWS account It is advisable this IAM user/role have admin access to the AWS account. Typically these credentials will be available only to the team managing the AWS account; hence the team deploying DataPull will need to coordinate with the team managing the AWS account. Have an S3 bucket <s3_bucket_name> (this bucket can be an existing bucket or a new bucket) that DataPull will use to store artifacts and logs under the folder datapull-opensource . The installation will use the name name as the prefix for new resources needed by DataPull, such as ECS service, Application load balancer, etc. Have available a VPC with ID <vpc_id> in the AWS region <region> Have available, two AWS VPC subnet ids <application_subnet_1> , <application_subnet_2> in the region VPC with ID <vpc_id> accessible to your clients. These subnets should ideally be private subnets, for the sake of security. Have available a security group with ID <application_security_group> that either your bastion node is a part of (for SSH-forwarding ports to your client network), or your client network has access to TCP ports 443 (if you wish to serve DataPull API over HTTPS), 8080 (if you wish to serve DataPull API over HTTP), 8088 (to access Spark Application Manager for monitoring DataPull), 20888 (to access Spark UI on EMR master for monitoring DataPull) if you are using DataPull in a dedicated/new VPC created by following these instructions , please use the ID of the security group ElasticMapReduce-Master-Private Have Docker installed on the machine used for deployment If you wish to serve the DataPull API over HTTPS (say, at https://datapull.yourdomain.com ), have available the ARN <load_balancer_certificate_arn> of an existing Certificate stored in AWS Certificate Manager that is valid for datapull.yourdomain.com the ability to create/modify a CNAME record for datapull.yourdomain.com to point to the load balancer URL for the DataPull API that will be created later in these instructions. Choose which environment <env> this DataPull deployment is for. Valid environments are dev test stage prod Pro-tip: Record the values of the variables in the pre-install steps, as a text file. Here is an example (with fake values)... <aws_admin_profile> : default <s3_bucket_name> : datatools-datapull <vpc_id> : vpc-071957e514a1fbd73 <region> : us-west-2 <application_subnet_1> : subnet-04903ae2649294308 <application_subnet_2> : subnet-0e10783af0de2513e <application_security_group> : sg-08beb77f882d5d2a2 <load_balancer_certificate_arn> : arn:aws:acm:us-west-2:771905877783:certificate/2b1d3729-aa6d-47d5-8902-d0f9e75b35a8 <env> : stage Installation Steps Edit master_application_config-\\<env>.yml This file is present in the root directory of the repo. Required attributes (excluding those with defaulted values) datapull.application.region: Put <region> here datapull.api.s3_bucket_name: Put <s3_bucket_name> here datapull.api.application_subnet_1: Put <application_subnet_1> here datapull.api.application_subnet_2: Put <application_subnet_2> here datapull.api.application_security_group: Put <application_security_group> here datapull.api.vpc_id: Put <vpc_id> here Optional attributes The following attributes need to be specified if you need DataPull to retrieve credentials from Hashicorp Vault to connect to your data stores. For this functionality to work, you need DataPull to run on AWS EMR (this is the default behaviour), or on a Spark cluster whose node(s) run on AWS EC2. a Hashicorp Vault cluster whose API is accessible at (e.g. https://myvault.mydomain.com:8200) credentials for the database cluster in the format { \"username\": \"<username>\", \"password\": \"<password>\" } stored as a static secret at the location <vault_url>/<static_secret_path_prefix>/<cluster>/<username> . For example, if you need DataPull to retrieve the credentials for the user mydbuser of the MySql cluster mycluster from the Vault cluster static secret path https://myvaultcluster:8200/v1/secret/mycluster/mydbuser , you will need to store the credentials using the cUrl command curl -X POST https://myvaultcluster:8200/v1/secret/mycluster/mydbuser -H 'X-Vault-Token: <valid Vault token>' -d '{ \"username\": \"mydbuser\", \"password\": \"<password>\" }' AWS EC2 Auth method set up on the Vault cluster, and the IAM Role of the Spark cluster mapped to a Vault policy that allows it to read the secret at the location <vault_url>/<static_secret_path_prefix>/<cluster>/<username> datapull.secretstore.vault.vault_url: Put <vault_url> here datapull.secretstore.vault.vault_nonce: Put a Client nonce here datapull.secretstore.vault.static_secret_path_prefix: Put <static_secret_path_prefix> here datapull.secretstore.vault.vault_path_login: Put the path to the login url for the AWS EC2 Auth endpoint. e.g. /v1/auth/<AWS Auth endpoint name>/login The following attribute needs to set if the DataPull API needs to be served over HTTPS. - datapull.api.load_balancer_certificate_arn: Put <load_balancer_certificate_arn> here The following attributes need to be set if you need DataPull to alert you if a job runs for too little time or if it runs too long. For this functionality, DataPull needs a SQL Server table <table> in the database <database> on the SQL Server instance <server> ; and an SQL Server login <login> with password <password> that can write to this table. If this table does not exist, please create this table with the following schema... CREATE TABLE [<table>]( [JobId] [nvarchar](max) NULL, [Portfolio] [nvarchar](max) NULL, [Product] [nvarchar](max) NULL, [MasterNode] [nvarchar](max) NULL, [Ec2Role] [nvarchar](max) NULL, [elapsedtime] [float] NOT NULL, [minexecutiontime] [bigint] NOT NULL, [maxexecutiontime] [bigint] NOT NULL, [status] [nvarchar](max) NULL, [InstantNow] [nvarchar](max) NULL, [processedflag] [bigint] NOT NULL, [EmailAddress] [nvarchar](max) NULL, [pipelineName] [nvarchar](max) NULL, [awsenv] [nvarchar](max) NULL, [BccEmailAddress] [nvarchar](max) NULL ) datapull.logger.mssql.server: Put <server> here datapull.logger.mssql.database: Put <database> here datapull.logger.mssql.login: Put <login> here datapull.logger.mssql.password: Put <password> here datapull.logger.mssql.table: Put <table> here The following attributes need to be set if you need DataPull to send an email report once each DataPull job completes, from the email address <emailaddress> through the SMTP server/relay <smtpserveraddress> datapull.logger.smtp.emailaddress: Put <emailaddress> here datapull.logger.smtp.smtpserveraddress: Put <smtpserveraddress> here The following attributes need to be set if you need DataPull to send an email report once each DataPull job completes, from the email address <email> using an existing SES instance that is accessiable using the AWS credentials <access_key> / <secret_key> datapull.logger.ses.email: Put <email> here datapull.logger.ses.access_key: Put <access_key> here datapull.logger.ses.secret_key: Put <secret_key> here The following attributes need to be set if you wish to run the EMR clusters for DataPull in any other security groups than the default EMR security groups that are created by AWS automatically when an EMR cluster is created datapull.emr.emr_security_group_master: Put <emr_security_group_master> here datapull.emr.emr_security_group_slave: Put <emr_security_group_slave> here datapull.emr.emr_security_group_service_access: Put <emr_security_group_service_access> here (optional) Oracle and Teradata support Please follow the instructions on this wiki to use Oracle and/or Teradata as data sources/destinations for DataPull. Create DataPull Infrastructure (optional) Add support for DataPull API to be served over HTTPS Edit the file /api/terraform/datapull_task/datapull_ecs.tf Update the resource aws_alb_listener with protocol as \"HTTPS\" (instead of \"HTTP\" ) port as 443 (instead of 8080 ) Create IAM User and Roles, with policies We recommend team managing the AWS account run this script from a terminal that has the AWS administrator credentials available as a profile <aws_admin_profile> in the Credential Profile Chain i.e. you should have the AWS credentials set in either your ~/.aws/credentials file if you're on a Mac or in your environment variables, or you are running this script from an AWS instance with an IAM Role that has administrator privileges </ul From the terminal at the root of the repo, run cd api/terraform/datapull_iam/ chmod +x create_user_and_roles.sh ./create_user_and_roles.sh <aws_admin_profile> <s3_bucket_name> <region> The script will create IAM Roles needed to run AWS Fargate and EMR an IAM User datapull_user whose access key and secret key are created as a new AWS profile datapull_user using aws configure CLI The IAM User datapull_user is used to install the remaining DataPull components like the AWS Fargate Cluster, the API load balancer, etc., so we recommend using datapull_user to set up a CI/CD pipeline for the next step. Create AWS Fargate API App and other AWS resources Ensure that there exists an AWS Profile datapull_user corresponding to the IAM User datapull_user created by the previous step > Pro-tip: You can verify this on a Mac by running the terminal command cat ~/.aws/credentials From the terminal (assuming you are using the same terminal session as the previous steps), run cd ../datapull_task/ If you are at the root of the repo, run cd api/terraform/datapull_task/ chmod +x ecs_deploy.sh ./ecs_deploy.sh <env> Browse to the DataPull API Swagger endpoint On AWS Console, navigate to Services > EC2 > Load Balancing > Load Balancers , and record the DNS name for DataPull's ALB (which is named <s3_bucket_name>-api-alb ) as <datpull_alb_internal_name> (e.g. internal-datatools-datapull-api-alb-507903794.us-west-2.elb.amazonaws.com ) On your browser, open the API endpoint URL http://<datpull_alb_internal_name>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST If you have selected private subnets for <application_subnet_1> , <application_subnet_2> , or if these subnets are accessible only through a bastion host, please port-forward <datpull_alb_internal_name>:8080 using the terminal command ssh -i \"<pem file for your ssh key>\" -N -L 8080:<datpull_alb_internal_name>:8080 <bastion user>@<bastion host address> and then open the API endpoint URL http://localhost:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST if you are using a bastion host, please ensure that bastion host is a part of <application_security_group> (along with other security groups it might need for access from your client network) if you are a EC2 instance in your public subnet as a poor man's bastion host as described in the AWS Account Setup wiki, then <bastion user> is ec2-user and <bastion host address> is the public IP of the EC2 instance Do your first DataPull Create a csv file at the S3 location s3://<s3_bucket_name>/datapull-opensource/data/firstdatapull/source/helloworld.csv with the following data hellofield,worldfield hello,world Post the following JSON input to the API endpoint url http://<datpull_alb_internal_name>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST . Please remember to replace <your_id@DOMAIN.com> and <s3_bucket_name> with valid data. { \"useremailaddress\": \"<your_id@DOMAIN.com>\", \"precisecounts\": true, \"migrations\": [ { \"sources\": [ { \"platform\": \"s3\", \"s3path\": \"<s3_bucket_name>/datapull-opensource/data/firstdatapull/source\", \"fileformat\": \"csv\", \"savemode\": \"Overwrite\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"<s3_bucket_name>/datapull-opensource/data/firstdatapull/destination\", \"fileformat\": \"json\", \"savemode\": \"Overwrite\" } } ], \"cluster\": { \"pipelinename\": \"firstdatapull\", \"awsenv\": \"dev\", \"portfolio\": \"Data Engineering Services\", \"terminateclusterafterexecution\": \"true\", \"product\": \"Data Engineering Data Tools\", \"ComponentInfo\": \"00000000-0000-0000-0000-000000000000\" } } In approximately 8 minutes (to account for the time taken for the ephemeral EMR cluster to spin up), you should get the data from the CSV converted into JSON and written to the S3 folder s3://<s3_bucket_name>/datapull-opensource/data/firstdatapull/destination/ The logs for each DataPull invocation are available at s3://<s3_bucket_name>/datapull-opensource/logs/DataPullHistory . The logs for each migration with DataPull invocations are available at s3://<s3_bucket_name>/datapull-opensource/logs/MigrationHistory If you had configured the anonymous SMTP server for DataPull to send you email reports, you would get an email report with the subject DataPull Report - firstdatapull (application_<random_numbers>) While the ephemeral EMR cluster is in the Running status, you can monitor the progress of the Spark job by browsing to the Spark UI at http://<ip of master node of EMR cluster>:8088 . Please read this wiki for more information If you have selected a private subnet for <application_subnet_1> or if this subnet is accessible only through a bastion host, please port-forward <ip of master node of EMR cluster>:8088 and <ip of master node of EMR cluster>:20888 using the terminal command ssh -i \"<pem file for your ssh key>\" -N -L 8088:<ip of master node of EMR cluster>:8088 -L 20888:<ip of master node of EMR cluster>:20888 <bastion user>@<bastion host address> and then open the API endpoint URL http://localhost:8088","title":"Installing DataPull on AWS Fargate and S3"},{"location":"install_on_aws/#deploying-datapull-on-aws-fargateecs-and-aws-emr","text":"This document helps you install DataPull on an Amazon AWS account, and run your first DataPull job of converting CSV data in AWS S3 to JSON data. In a nutshell, deploying DataPull to an AWS Account creates three IAM Roles datapull_task_role , datapull_task_execution_role for running the DataPull REST API on AWS Fargate emr_ec2_datapull_role for running ephemeral AWS EMR clusters creates an IAM User datapull_user temporarily for the purpose of installing the following DataPull components an AWS Fargate service datapull-web-api with an associated image in AWS ECR, and AWS Application Load Balancer (ALB) an AWS CloudWatch Log Group datapull_cloudwatch_log_group and associated log stream stores DataPull JAR for EMR, job history, EMR logs in an existing AWS S3 bucket Note: <xyz> denotes a variable/attribute named xyz in the DataPull configuration file","title":"Deploying DataPull on AWS Fargate/ECS and AWS EMR"},{"location":"install_on_aws/#pre-install-steps","text":"Clone/download the master branch of this repo for fargate based install or master_ecs_ec2 branch if you want to do ecs based install. Have available, the AWS Profile <aws_admin_profile> of an IAM user/role that can create IAM users and IAM roles in your AWS account It is advisable this IAM user/role have admin access to the AWS account. Typically these credentials will be available only to the team managing the AWS account; hence the team deploying DataPull will need to coordinate with the team managing the AWS account. Have an S3 bucket <s3_bucket_name> (this bucket can be an existing bucket or a new bucket) that DataPull will use to store artifacts and logs under the folder datapull-opensource . The installation will use the name name as the prefix for new resources needed by DataPull, such as ECS service, Application load balancer, etc. Have available a VPC with ID <vpc_id> in the AWS region <region> Have available, two AWS VPC subnet ids <application_subnet_1> , <application_subnet_2> in the region VPC with ID <vpc_id> accessible to your clients. These subnets should ideally be private subnets, for the sake of security. Have available a security group with ID <application_security_group> that either your bastion node is a part of (for SSH-forwarding ports to your client network), or your client network has access to TCP ports 443 (if you wish to serve DataPull API over HTTPS), 8080 (if you wish to serve DataPull API over HTTP), 8088 (to access Spark Application Manager for monitoring DataPull), 20888 (to access Spark UI on EMR master for monitoring DataPull) if you are using DataPull in a dedicated/new VPC created by following these instructions , please use the ID of the security group ElasticMapReduce-Master-Private Have Docker installed on the machine used for deployment If you wish to serve the DataPull API over HTTPS (say, at https://datapull.yourdomain.com ), have available the ARN <load_balancer_certificate_arn> of an existing Certificate stored in AWS Certificate Manager that is valid for datapull.yourdomain.com the ability to create/modify a CNAME record for datapull.yourdomain.com to point to the load balancer URL for the DataPull API that will be created later in these instructions. Choose which environment <env> this DataPull deployment is for. Valid environments are dev test stage prod Pro-tip: Record the values of the variables in the pre-install steps, as a text file. Here is an example (with fake values)... <aws_admin_profile> : default <s3_bucket_name> : datatools-datapull <vpc_id> : vpc-071957e514a1fbd73 <region> : us-west-2 <application_subnet_1> : subnet-04903ae2649294308 <application_subnet_2> : subnet-0e10783af0de2513e <application_security_group> : sg-08beb77f882d5d2a2 <load_balancer_certificate_arn> : arn:aws:acm:us-west-2:771905877783:certificate/2b1d3729-aa6d-47d5-8902-d0f9e75b35a8 <env> : stage","title":"Pre-install steps"},{"location":"install_on_aws/#installation-steps","text":"","title":"Installation Steps"},{"location":"install_on_aws/#edit-master_application_config-envyml","text":"This file is present in the root directory of the repo.","title":"Edit master_application_config-\\&lt;env>.yml"},{"location":"install_on_aws/#required-attributes-excluding-those-with-defaulted-values","text":"datapull.application.region: Put <region> here datapull.api.s3_bucket_name: Put <s3_bucket_name> here datapull.api.application_subnet_1: Put <application_subnet_1> here datapull.api.application_subnet_2: Put <application_subnet_2> here datapull.api.application_security_group: Put <application_security_group> here datapull.api.vpc_id: Put <vpc_id> here","title":"Required attributes (excluding those with defaulted values)"},{"location":"install_on_aws/#optional-attributes","text":"The following attributes need to be specified if you need DataPull to retrieve credentials from Hashicorp Vault to connect to your data stores. For this functionality to work, you need DataPull to run on AWS EMR (this is the default behaviour), or on a Spark cluster whose node(s) run on AWS EC2. a Hashicorp Vault cluster whose API is accessible at (e.g. https://myvault.mydomain.com:8200) credentials for the database cluster in the format { \"username\": \"<username>\", \"password\": \"<password>\" } stored as a static secret at the location <vault_url>/<static_secret_path_prefix>/<cluster>/<username> . For example, if you need DataPull to retrieve the credentials for the user mydbuser of the MySql cluster mycluster from the Vault cluster static secret path https://myvaultcluster:8200/v1/secret/mycluster/mydbuser , you will need to store the credentials using the cUrl command curl -X POST https://myvaultcluster:8200/v1/secret/mycluster/mydbuser -H 'X-Vault-Token: <valid Vault token>' -d '{ \"username\": \"mydbuser\", \"password\": \"<password>\" }' AWS EC2 Auth method set up on the Vault cluster, and the IAM Role of the Spark cluster mapped to a Vault policy that allows it to read the secret at the location <vault_url>/<static_secret_path_prefix>/<cluster>/<username> datapull.secretstore.vault.vault_url: Put <vault_url> here datapull.secretstore.vault.vault_nonce: Put a Client nonce here datapull.secretstore.vault.static_secret_path_prefix: Put <static_secret_path_prefix> here datapull.secretstore.vault.vault_path_login: Put the path to the login url for the AWS EC2 Auth endpoint. e.g. /v1/auth/<AWS Auth endpoint name>/login The following attribute needs to set if the DataPull API needs to be served over HTTPS. - datapull.api.load_balancer_certificate_arn: Put <load_balancer_certificate_arn> here The following attributes need to be set if you need DataPull to alert you if a job runs for too little time or if it runs too long. For this functionality, DataPull needs a SQL Server table <table> in the database <database> on the SQL Server instance <server> ; and an SQL Server login <login> with password <password> that can write to this table. If this table does not exist, please create this table with the following schema... CREATE TABLE [<table>]( [JobId] [nvarchar](max) NULL, [Portfolio] [nvarchar](max) NULL, [Product] [nvarchar](max) NULL, [MasterNode] [nvarchar](max) NULL, [Ec2Role] [nvarchar](max) NULL, [elapsedtime] [float] NOT NULL, [minexecutiontime] [bigint] NOT NULL, [maxexecutiontime] [bigint] NOT NULL, [status] [nvarchar](max) NULL, [InstantNow] [nvarchar](max) NULL, [processedflag] [bigint] NOT NULL, [EmailAddress] [nvarchar](max) NULL, [pipelineName] [nvarchar](max) NULL, [awsenv] [nvarchar](max) NULL, [BccEmailAddress] [nvarchar](max) NULL ) datapull.logger.mssql.server: Put <server> here datapull.logger.mssql.database: Put <database> here datapull.logger.mssql.login: Put <login> here datapull.logger.mssql.password: Put <password> here datapull.logger.mssql.table: Put <table> here The following attributes need to be set if you need DataPull to send an email report once each DataPull job completes, from the email address <emailaddress> through the SMTP server/relay <smtpserveraddress> datapull.logger.smtp.emailaddress: Put <emailaddress> here datapull.logger.smtp.smtpserveraddress: Put <smtpserveraddress> here The following attributes need to be set if you need DataPull to send an email report once each DataPull job completes, from the email address <email> using an existing SES instance that is accessiable using the AWS credentials <access_key> / <secret_key> datapull.logger.ses.email: Put <email> here datapull.logger.ses.access_key: Put <access_key> here datapull.logger.ses.secret_key: Put <secret_key> here The following attributes need to be set if you wish to run the EMR clusters for DataPull in any other security groups than the default EMR security groups that are created by AWS automatically when an EMR cluster is created datapull.emr.emr_security_group_master: Put <emr_security_group_master> here datapull.emr.emr_security_group_slave: Put <emr_security_group_slave> here datapull.emr.emr_security_group_service_access: Put <emr_security_group_service_access> here","title":"Optional attributes"},{"location":"install_on_aws/#optional-oracle-and-teradata-support","text":"Please follow the instructions on this wiki to use Oracle and/or Teradata as data sources/destinations for DataPull.","title":"(optional) Oracle and Teradata support"},{"location":"install_on_aws/#create-datapull-infrastructure","text":"","title":"Create DataPull Infrastructure"},{"location":"install_on_aws/#optional-add-support-for-datapull-api-to-be-served-over-https","text":"Edit the file /api/terraform/datapull_task/datapull_ecs.tf Update the resource aws_alb_listener with protocol as \"HTTPS\" (instead of \"HTTP\" ) port as 443 (instead of 8080 )","title":"(optional) Add support for DataPull API to be served over HTTPS"},{"location":"install_on_aws/#create-iam-user-and-roles-with-policies","text":"We recommend team managing the AWS account run this script from a terminal that has the AWS administrator credentials available as a profile <aws_admin_profile> in the Credential Profile Chain i.e. you should have the AWS credentials set in either your ~/.aws/credentials file if you're on a Mac or in your environment variables, or you are running this script from an AWS instance with an IAM Role that has administrator privileges </ul From the terminal at the root of the repo, run cd api/terraform/datapull_iam/ chmod +x create_user_and_roles.sh ./create_user_and_roles.sh <aws_admin_profile> <s3_bucket_name> <region> The script will create IAM Roles needed to run AWS Fargate and EMR an IAM User datapull_user whose access key and secret key are created as a new AWS profile datapull_user using aws configure CLI The IAM User datapull_user is used to install the remaining DataPull components like the AWS Fargate Cluster, the API load balancer, etc., so we recommend using datapull_user to set up a CI/CD pipeline for the next step.","title":"Create IAM User and Roles, with policies"},{"location":"install_on_aws/#create-aws-fargate-api-app-and-other-aws-resources","text":"Ensure that there exists an AWS Profile datapull_user corresponding to the IAM User datapull_user created by the previous step > Pro-tip: You can verify this on a Mac by running the terminal command cat ~/.aws/credentials From the terminal (assuming you are using the same terminal session as the previous steps), run cd ../datapull_task/ If you are at the root of the repo, run cd api/terraform/datapull_task/ chmod +x ecs_deploy.sh ./ecs_deploy.sh <env>","title":"Create AWS Fargate API App and other AWS resources"},{"location":"install_on_aws/#browse-to-the-datapull-api-swagger-endpoint","text":"On AWS Console, navigate to Services > EC2 > Load Balancing > Load Balancers , and record the DNS name for DataPull's ALB (which is named <s3_bucket_name>-api-alb ) as <datpull_alb_internal_name> (e.g. internal-datatools-datapull-api-alb-507903794.us-west-2.elb.amazonaws.com ) On your browser, open the API endpoint URL http://<datpull_alb_internal_name>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST If you have selected private subnets for <application_subnet_1> , <application_subnet_2> , or if these subnets are accessible only through a bastion host, please port-forward <datpull_alb_internal_name>:8080 using the terminal command ssh -i \"<pem file for your ssh key>\" -N -L 8080:<datpull_alb_internal_name>:8080 <bastion user>@<bastion host address> and then open the API endpoint URL http://localhost:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST if you are using a bastion host, please ensure that bastion host is a part of <application_security_group> (along with other security groups it might need for access from your client network) if you are a EC2 instance in your public subnet as a poor man's bastion host as described in the AWS Account Setup wiki, then <bastion user> is ec2-user and <bastion host address> is the public IP of the EC2 instance","title":"Browse to the DataPull API Swagger endpoint"},{"location":"install_on_aws/#do-your-first-datapull","text":"Create a csv file at the S3 location s3://<s3_bucket_name>/datapull-opensource/data/firstdatapull/source/helloworld.csv with the following data hellofield,worldfield hello,world Post the following JSON input to the API endpoint url http://<datpull_alb_internal_name>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST . Please remember to replace <your_id@DOMAIN.com> and <s3_bucket_name> with valid data. { \"useremailaddress\": \"<your_id@DOMAIN.com>\", \"precisecounts\": true, \"migrations\": [ { \"sources\": [ { \"platform\": \"s3\", \"s3path\": \"<s3_bucket_name>/datapull-opensource/data/firstdatapull/source\", \"fileformat\": \"csv\", \"savemode\": \"Overwrite\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"<s3_bucket_name>/datapull-opensource/data/firstdatapull/destination\", \"fileformat\": \"json\", \"savemode\": \"Overwrite\" } } ], \"cluster\": { \"pipelinename\": \"firstdatapull\", \"awsenv\": \"dev\", \"portfolio\": \"Data Engineering Services\", \"terminateclusterafterexecution\": \"true\", \"product\": \"Data Engineering Data Tools\", \"ComponentInfo\": \"00000000-0000-0000-0000-000000000000\" } } In approximately 8 minutes (to account for the time taken for the ephemeral EMR cluster to spin up), you should get the data from the CSV converted into JSON and written to the S3 folder s3://<s3_bucket_name>/datapull-opensource/data/firstdatapull/destination/ The logs for each DataPull invocation are available at s3://<s3_bucket_name>/datapull-opensource/logs/DataPullHistory . The logs for each migration with DataPull invocations are available at s3://<s3_bucket_name>/datapull-opensource/logs/MigrationHistory If you had configured the anonymous SMTP server for DataPull to send you email reports, you would get an email report with the subject DataPull Report - firstdatapull (application_<random_numbers>) While the ephemeral EMR cluster is in the Running status, you can monitor the progress of the Spark job by browsing to the Spark UI at http://<ip of master node of EMR cluster>:8088 . Please read this wiki for more information If you have selected a private subnet for <application_subnet_1> or if this subnet is accessible only through a bastion host, please port-forward <ip of master node of EMR cluster>:8088 and <ip of master node of EMR cluster>:20888 using the terminal command ssh -i \"<pem file for your ssh key>\" -N -L 8088:<ip of master node of EMR cluster>:8088 -L 20888:<ip of master node of EMR cluster>:20888 <bastion user>@<bastion host address> and then open the API endpoint URL http://localhost:8088","title":"Do your first DataPull"},{"location":"monitor_spark_ui/","text":"DataPull internally uses Spark to move data across platforms. DataPull jobs can be monitored using the Spark UI. Spark UI is available only while the EMR cluster is in running status, and has a step in running status. Prerequisites Prerequisite for environments outside of Vrbo and EG Data Platform The Spark UI is a website that is hosted on the master node of the EMR cluster running DataPull. If most cases, the IP/DNS Name of the EMR cluster's master is inaccessible from the client machine of the user; since there is no peering/VPN/Direct Connect set up from the user's network to the VPC of the EMR Cluster. Hence, it is necessary to use SSH Tunnelling to access this website on the user's client machine. Option 1 (recommended for more technical users) Use an SSH client to connect to the master node, configure SSH tunneling with local port forwarding, and use an Internet browser to open web interfaces hosted on the master node. This method allows you to configure web interface access without using a SOCKS proxy. For more information, refer to https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel-local.html Option 2 (recommended for new users) Use an SSH client to connect to the master node, configure SSH tunneling with dynamic port forwarding, and configure your Internet browser to use an add-on such as FoxyProxy or SwitchySharp to manage your SOCKS proxy settings. This method allows you to automatically filter URLs based on text patterns and to limit the proxy settings to domains that match the form of the master node's DNS name. The browser add-on automatically handles turning the proxy on and off when you switch between viewing websites hosted on the master node and those on the Internet. For more information, refer to https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html Steps to access Spark UI Open the AWS console for environment in which the datapull job is running. select EMR under the services tab. Select the EMR cluster for the corresponding datapull job. The EMR Cluster will be named <environment>-emr-<DataPull Pipeline Name>-pipeline Click the Hardware tab and Click the ID of the Master node which can recongnised under the Node Type & name tab. Copy the <Master Node Private IP Address> to your clipboard. Open a new browser window/tab and browse to http://<Master Node Private IP Address>:8088 which will open the YARN ResourceManager Click on the link which says ApplicationMaster under the Tracking UI. If the page redirection fails with This page isn\u2019t working or DNS Error , and if the <Master Node Private IP Address> is 1.2.3.4 then replace ip-1-2-3-4.ec2.internal with 1.2.3.4 in the URL You are now at th Spark UI. For understanding more on analyzing Spark UI, refer to Understanding your Apache Spark Application Through Visualization To understand all the available web interfaces which can be accessed through <Master Node Private IP Address> , refer to this page","title":"Monitor DataPull jobs using Spark UI"},{"location":"monitor_spark_ui/#prerequisites","text":"","title":"Prerequisites"},{"location":"monitor_spark_ui/#prerequisite-for-environments-outside-of-vrbo-and-eg-data-platform","text":"The Spark UI is a website that is hosted on the master node of the EMR cluster running DataPull. If most cases, the IP/DNS Name of the EMR cluster's master is inaccessible from the client machine of the user; since there is no peering/VPN/Direct Connect set up from the user's network to the VPC of the EMR Cluster. Hence, it is necessary to use SSH Tunnelling to access this website on the user's client machine.","title":"Prerequisite for environments outside of Vrbo and EG Data Platform"},{"location":"monitor_spark_ui/#option-1-recommended-for-more-technical-users","text":"Use an SSH client to connect to the master node, configure SSH tunneling with local port forwarding, and use an Internet browser to open web interfaces hosted on the master node. This method allows you to configure web interface access without using a SOCKS proxy. For more information, refer to https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel-local.html","title":"Option 1 (recommended for more technical users)"},{"location":"monitor_spark_ui/#option-2-recommended-for-new-users","text":"Use an SSH client to connect to the master node, configure SSH tunneling with dynamic port forwarding, and configure your Internet browser to use an add-on such as FoxyProxy or SwitchySharp to manage your SOCKS proxy settings. This method allows you to automatically filter URLs based on text patterns and to limit the proxy settings to domains that match the form of the master node's DNS name. The browser add-on automatically handles turning the proxy on and off when you switch between viewing websites hosted on the master node and those on the Internet. For more information, refer to https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html","title":"Option 2 (recommended for new users)"},{"location":"monitor_spark_ui/#steps-to-access-spark-ui","text":"Open the AWS console for environment in which the datapull job is running. select EMR under the services tab. Select the EMR cluster for the corresponding datapull job. The EMR Cluster will be named <environment>-emr-<DataPull Pipeline Name>-pipeline Click the Hardware tab and Click the ID of the Master node which can recongnised under the Node Type & name tab. Copy the <Master Node Private IP Address> to your clipboard. Open a new browser window/tab and browse to http://<Master Node Private IP Address>:8088 which will open the YARN ResourceManager Click on the link which says ApplicationMaster under the Tracking UI. If the page redirection fails with This page isn\u2019t working or DNS Error , and if the <Master Node Private IP Address> is 1.2.3.4 then replace ip-1-2-3-4.ec2.internal with 1.2.3.4 in the URL You are now at th Spark UI. For understanding more on analyzing Spark UI, refer to Understanding your Apache Spark Application Through Visualization To understand all the available web interfaces which can be accessed through <Master Node Private IP Address> , refer to this page","title":"Steps to access Spark UI"},{"location":"oracle_teradata_support/","text":"Support for Oracle and Teradata sources/destinations for DataPull To use Oracle and Teradata as sources/destinations for DataPull, you need to manually download their driver JARs from their companys' websites and add them as dependencies in the /core/pom.xml file of this repo. Steps to download Oracle ojdbc jar Go to the URL https://www.oracle.com/database/technologies/appdev/jdbc-ucp-19c-downloads.html Accept the license agreement Oracle will ask you to create an account if you don't have one already Download the latest version of the ojdbc JAR ( ojdbc10.jar as of 12/30/2019) Steps to download Teradata jar Go to the URL https://downloads.teradata.com/download/connectivity/jdbc-driver. For downloading Teradata jar, you will need to create an account on the Teradata website The download will be available in either .tar or .zip format. Download the latest archive file whose name will be in the format TeraJDBC__indep_indep_{version}.zip The archive file will contain two jar files terajdbc4.jar and tdgssconfig.jar . Both jars are needed for Teradata jdbc functionality Steps to include Oracle into the project Run this command to include Oracle jar into maven repo. Run this command from folder where Oracle jars are present. Change the value of -Dversion=11.2.0.3 in the command according to downloaded jar version. Command : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=ojdbc6.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.oracle</groupId> <artifactId>ojdbc6</artifactId> <version>{version}</version> </dependency> Steps to include Teradata into the project Run these commands to include Teradata jars into maven repo. Run this command from folder where Teradata jars are present. Change the value of -Dversion=16.20.00.10 in the command according to downloaded jar version. Commands : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=terajdbc4.jar -DgroupId=com.teradata -DartifactId=terajdbc4 -Dversion=16.20.00.10 -Dpackaging=jar docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=tdgssconfig.jar -DgroupId=com.teradata -DartifactId=tdgssconfig -Dversion=16.20.00.10 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.teradata</groupId> <artifactId>terajdbc4</artifactId> <version>{version}</version> </dependency> <dependency> <groupId>com.teradata</groupId> <artifactId>tdgssconfig</artifactId> <version>{version}</version> </dependency>","title":"Add support for Oracle and Teradata"},{"location":"oracle_teradata_support/#support-for-oracle-and-teradata-sourcesdestinations-for-datapull","text":"To use Oracle and Teradata as sources/destinations for DataPull, you need to manually download their driver JARs from their companys' websites and add them as dependencies in the /core/pom.xml file of this repo.","title":"Support for Oracle and Teradata sources/destinations for DataPull"},{"location":"oracle_teradata_support/#steps-to-download-oracle-ojdbc-jar","text":"Go to the URL https://www.oracle.com/database/technologies/appdev/jdbc-ucp-19c-downloads.html Accept the license agreement Oracle will ask you to create an account if you don't have one already Download the latest version of the ojdbc JAR ( ojdbc10.jar as of 12/30/2019)","title":"Steps to download Oracle ojdbc jar"},{"location":"oracle_teradata_support/#steps-to-download-teradata-jar","text":"Go to the URL https://downloads.teradata.com/download/connectivity/jdbc-driver. For downloading Teradata jar, you will need to create an account on the Teradata website The download will be available in either .tar or .zip format. Download the latest archive file whose name will be in the format TeraJDBC__indep_indep_{version}.zip The archive file will contain two jar files terajdbc4.jar and tdgssconfig.jar . Both jars are needed for Teradata jdbc functionality","title":"Steps to download Teradata jar"},{"location":"oracle_teradata_support/#steps-to-include-oracle-into-the-project","text":"Run this command to include Oracle jar into maven repo. Run this command from folder where Oracle jars are present. Change the value of -Dversion=11.2.0.3 in the command according to downloaded jar version. Command : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=ojdbc6.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.oracle</groupId> <artifactId>ojdbc6</artifactId> <version>{version}</version> </dependency>","title":"Steps to include Oracle into the project"},{"location":"oracle_teradata_support/#steps-to-include-teradata-into-the-project","text":"Run these commands to include Teradata jars into maven repo. Run this command from folder where Teradata jars are present. Change the value of -Dversion=16.20.00.10 in the command according to downloaded jar version. Commands : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=terajdbc4.jar -DgroupId=com.teradata -DartifactId=terajdbc4 -Dversion=16.20.00.10 -Dpackaging=jar docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=tdgssconfig.jar -DgroupId=com.teradata -DartifactId=tdgssconfig -Dversion=16.20.00.10 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.teradata</groupId> <artifactId>terajdbc4</artifactId> <version>{version}</version> </dependency> <dependency> <groupId>com.teradata</groupId> <artifactId>tdgssconfig</artifactId> <version>{version}</version> </dependency>","title":"Steps to include Teradata into the project"},{"location":"transformation/","text":"{ \"useremailaddress\": \"your_email_address\", \"migrations\": [ { \"source\": { \"platform\": \"elastic\", \"clustername\":\"cluster_name\", \"port\": \"9201\", \"login\": \"user_id\", \"password\": \"password\", \"index\": \"test\", \"type\": \"docs\", \"version\": \"6.3.0\", \"alias\":\"U\" }, \"sql\": { \"query\": \"SELECT id, name, DATE_FORMAT(current_date(), \\\"y-MM-dd HH:mm:ss.SSS\\\") as date FROM U\" }, \"destination\": { \"platform\": \"mssql\", \"awsenv\": \"test\", \"server\": \"server_name\", \"database\": \"database\", \"table\": \"destination_table\", \"login\": \"user_id\", \"password\": \"password\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } }","title":"Sample input JSON"}]}