{
  "comment_general": "Throughout this document, you will encounter elements named comment_* . These are comments meant to hep you understand the structure of this JSON. Feel free to remove them if you wish.",
  "comment_useremailaddress": "optional. If provided, an email report is sent to this address",
  "useremailaddress": "YOUR_ID@DOMAIN.com",
  "comment_precisecounts": "optional. If provided true, then we persist the dataframe to provide the accurate counts which will add additional time for the datapull",
  "precisecounts": false,
  "migration_failure_threshold": "1",
  "comment_failure_threshold": "Number of acceptable failures in a given set of migrations",
  "comment_jsoninputfile": "optional. The EMR API rejects JSON inputs that are too long. In that event (and/or to maintain versioned inputs) you can provide this JSON as a .json file uploaded in S3, using this element",
  "jsoninputfile": {
    "s3path": "bucketname/folder/input.json",
    "comment_awsaccesskeyid": "optional, if EMR runs in same environment as S3. Please replace this expired id with your own.",
    "awsaccesskeyid": "AWS_ACCESS_KEY",
    "comment_awssecretaccesskey": "optional, if EMR runs in same environment as S3. Please replace this expired secret with your own.",
    "awssecretaccesskey": "AWS_SECRET_ACCESS_KEY"
  },
  "comment_sparkjarfile": "optional. This allows you to customize the logic of the scala code in this repo and execute the uploaded cusom jar file in S3",
  "sparkjarfile": "s3://bucketname/folder/your_custom_jar.jar",
  "comment_parallelmigrations": "optional. If this flag is not set or if it is set to false, the migrations will happen in series. At present, only 4 migrations can happen in parallel at a given time if this flag is set to true",
  "parallelmigrations": false,
  "reportcounts": "default value for this is true and if you want to process the job a bit faster at the cost of not having counts in the email",
  "no_of_retries": "The number of retries the tasks within a job has to be tried in case of a failure",
  "comment_failureemailaddress": "optional. If provided, an failure email report is sent to this address in case of job failures and for scheduled prod jobs can be paged/any default email in the properties file",
  "failureemailaddress": "yourid@DOMAIN.com",
  "migrations": [
    {
      "comment_source_destination": "source and destination are required elements in this JSON. The format varies, based on the platform. The elements source_* and destination_* below show examples of sources and destinations of various platforms supported by Data Migration. The format of a platform is the ssame regardless of whether it is used as a source or a destination",
      "source_file": {
        "platform": "filesystem",
        "path": "/FOLDER/SUB_FOLDER/",
        "comment_fileformat": "Supported formats are parquet,json,csv,orc,avro. Tab-delimited files can be read by cboosing the fileformat as csv and setting the delimiter to \t",
        "fileformat": "parquet",
        "comment_groupbyfields": "optional. Applies only to destinations. If specified, the records are group in nested subfolders named field1=value etc",
        "groupbyfields": "field1,field2",
        "comment_delimiter": "Deprecated in favor of sparkoptions.delimiter. Optional, defaulted to comma(,). User \\t for tab-delimited files. Applies only for csv fileformat",
        "delimiter": ",",
        "comment_charset": "charset is for specifying the type whether it is utf-16/utf-8 and it is optional and defaulted to utf-8",
        "charset": "utf-16",
        "comment_isstream": "Optional, defaulted to false. Set to true if you want changes to the filesystem to be streamed instead of read as a batch, as input. If set to true, do not use the built-in scheduler since the spark process will run continuously.",
        "isstream": false,
        "comment_sparkoptions": "optional set of spark.cassandra options. these options will override equivalent parameters if both are provided. For example, delimiter specificed in sparkoptions will override delimiter specified (and deprecated) above",
        "sparkoptions": {
          "comment_checkpointLocation": "For some output sinks where the end-to-end fault-tolerance can be guaranteed, specify the location where the system will write all the checkpoint information. This should be a directory in an HDFS-compatible fault-tolerant file system. ref https://spark.apache.org/docs/2.4.7/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing",
          "checkpointLocation": "s3a://S3BUCKET_AND_PATH or /LOCALHDFSPATH",
          "comment_delimiter": "Optional, defaulted to comma(,). Use \\t for tab-delimited files. Applies only for csv fileformat",
          "delimiter": "\t"
        }
      },
      "source_s3": {
        "platform": "s3",
        "s3path": "BUCKET_NAME/FOLDER_PATH",
        "comment_fileformat": "Supported formats are parquet,json,csv,orc,avro. Tab-delimited files can be read by cboosing the fileformat as csv and setting the delimiter to \t",
        "fileformat": "parquet",
        "comment_groupbyfields": "optional. Applies only to destinations. If specified, the records are group in nested subfolders named field1=value etc",
        "groupbyfields": "field1,field2",
        "comment_awsaccesskeyid": "optional, if EMR runs in same environment as S3. Please replace this expired id with your own.",
        "awsaccesskeyid": "AWS_ACCESS_KEY",
        "comment_awssecretaccesskey": "optional, if EMR runs in same environment as S3. Please replace this expired secret with your own.",
        "awssecretaccesskey": "AWS_SECRET_ACCESS_KEY",
        "comment_s3_service_endpoint": "Optional. Allows the setting of custom AWS Service Endpoint. This is useful when reading/writing to IBM Object Storage etc.",
        "s3_service_endpoint": "http://s3.us-south.aws.service.endpoint",
        "comment_enable_s3_bucket_owner_full_control": "Optional, defaults to true. If set or omitted, this sets the default ACL to BucketOwnerFullControl when writing data to S3. This is useful when doing cross-account S3 writes, and you want to allow the detination S3 bucket's owner to have full access to the written data. Set this to false when reading/writing to IBM Object Storage",
        "enable_s3_bucket_owner_full_control": true,
        "comment_savemode": "Optional, This should be one of the Append/Overwrite/ErrorIfExists",
        "savemode": "Append",
        "comment_delimiter": "Accepted values are ',','\t' and default value is comma(,)",
        "delimiter": "\t",
        "comment_mergeschema": "Optional, this is to merge different schemas when reading parquet files. the default value for this is false",
        "mergeschema": false,
        "comment_coalescefilecount": "Optional, Specify the number of partitions, which determine the number of files in the destination folder. Applies only to destinations, not sources",
        "coalescefilecount": null,
        "comment_enable_server_side_encryption": "Optional, to access the buckets which have server side encryption enabled with AES 256 to be set true. Default: false",
        "enable_server_side_encryption": "false",
        "comment_post_migrate_command": "This is to execute copy/delete operation after the desired migration to s3",
        "post_migrate_command": {
          "comment_operation": "This field will take options either copy/delete.",
          "operation": "copy",
          "commentsources3path": "this is required if we are doing a copy operation from which we want to copy data from as part of the post migration command",
          "sources3path": "bucket/folders",
          "comment_destinations3path": "this is required if we are doing a copy operation from which we want to copy data to as part of the post migration command",
          "destinations3path": "bucket/folders",
          "comment_partitioned": "if the data is partitioned by any key then it should be true",
          "partitioned": true,
          "comment_overwrite": "if the data to be over written then it should be true",
          "overwrite": true,
          "comment_removesource": "If we want to remove the source s3 folder after copying as part of the post migration then it will be true",
          "removesource": false,
          "comment_s3region": "optional, the default region is us-east-1 and can use this if the bucket is in different region",
          "s3region": "us-east-1"
        },
        "post_migrate_commands": [
          {
            "comment_operation": "This field will take options either copy/delete.",
            "operation": "copy",
            "commentsources3path": "this is required if we are doing a copy operation from which we want to copy data from as part of the post migration command",
            "sources3path": "BUCKET_NAME/FOLDER_PATH",
            "comment_destinations3path": "this is required if we are doing a copy operation from which we want to copy data to as part of the post migration command",
            "destinations3path": "BUCKET_NAME/FOLDER_PATH",
            "comment_partitioned": "if the data is partitioned by any key then it should be true",
            "partitioned": true,
            "comment_overwrite": "if the data to be over written then it should be true",
            "overwrite": true,
            "comment_removesource": "If we want to remove the source s3 folder after copying as part of the post migration then it will be true",
            "removesource": false
          },
          {
            "comment_operation": "This field will take options either copy/delete.",
            "operation": "delete",
            "comment_s3path": "this is required if we are doing a copy operation from which we want to copy data from as part of the post migration command",
            "s3path": "bucket/folders",
            "comment_s3region": "optional, the default region is us-east-1 and can use this if the bucket is in different region",
            "s3region": "us-east-1"
          }
        ]
      },
      "comment_sql": "Optional, if Any complex sql queries to be executed on multiple sources then this can be used.",
      "sql": {
        "comment_query": "The sql query to be executed on the sources mentioned.",
        "query": "any sql query."
      },
      "source_cassandra": {
        "platform": "cassandra",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "comment_cluster": "This element accepts both IPs and Consul DNS names. Consul DNS names are required if Vault integration is used to fetch the credentials",
        "cluster": "IP_ADDR_OR_DNS_NAME",
        "keyspace": "YOURKEYSPACE_NAME",
        "table": "TABLE_NAME",
        "login": "YOURKEYSPACENAME_APPUSER",
        "comment_password": "optional. Required if Vault integration is not used",
        "password": "PASSWORD",
        "comment_local_dc": "optional. Thi allows EMR to read/write from a specific datacenter/region alone",
        "local_dc": "LOCAL_DATACENTER",
        "Comment_pre_migrate_commands": "optional, an array of pre migration commands can be executed.",
        "pre_migrate_commands": [
          {
            "query": "use demo"
          },
          {
            "query": "use demo"
          }
        ],
        "comment_pre_migrate_command": "optional. CQL command to run on the platform prior to the data migration. Commands like TRUNCATE are NOT RECOMMENDED to run unless you know what you are doing",
        "pre_migrate_command": "use demo;",
        "comment_post_migrate_command": "optional. CQL command to run on the platform after data migration. Commands like TRUNCATE are NOT RECOMMENDED to run unless you know what you are doing",
        "post_migrate_command": "use demo;",
        "comment_post_migrate_commands": "optional, An array of post migration commands can be executed",
        "post_migrate_commands": [
          {
            "query": "use demo"
          },
          {
            "query": "use demo"
          }
        ],
        "comment_sparkoptions": "optional set of spark.cassandra options. \"spark.cassandra.connection.local_dc\" will override \"local_dc\" if both are provided",
        "sparkoptions": {
          "spark.input.consistency.level": "LOCAL_QUORUM",
          "comment_spark.cassandra.connection.ssl.*": "required if cassandra cluster accepts only SSL connections",
          "spark.cassandra.connection.ssl.enabled": true,
          "comment_spark.cassandra.connection.ssl.trustStore.path": "The truststore file should be available to all nodes of the cluster. For EMR clusters spun up using Data Pull, the truststore is injected into all nodes at /mnt/bootstrapfiles/client-server.jks",
          "spark.cassandra.connection.ssl.trustStore.path": "/mnt/bootstrapfiles/client-server.jks",
          "spark.cassandra.connection.ssl.trustStore.password": "TRUSTUSTORE_PASSWORD"
        }
      },
      "source_rdbms": {
        "comment_platform": "other supported rdbms platforms are mssql, mysql, postgres. These sources have the same object structure as mssql shown below",
        "platform": "mssql",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "comment_server": "This element accepts both IPs and CNAMEs e.g. mydbserver.domain.internal. CNAMEs are required if Vault integration is used to fetch the credentials",
        "server": "IP_ADDR_OR_DNS_NAME",
        "database": "DATABASE_NAME",
        "table": "SCHEMA_NAME.TABLE_NAME",
        "comment_querys3sqlfile": "optional. SQL Query provided instrea of a source table, in a .sql file uploaded to S3. If this is provided along with the table element, this takes priority",
        "querys3sqlfile": {
          "s3path": "BUCKET_NAME/FOLDER_PATH",
          "comment_awsaccesskeyid": "optional, if EMR runs in same environment as S3. Please replace this expired id with your own.",
          "awsaccesskeyid": "AWS_ACCESS_KEY",
          "comment_awssecretaccesskey": "optional, if EMR runs in same environment as S3. Please replace this expired secret with your own.",
          "awssecretaccesskey": "AWS_SECRET_ACCESS_KEY"
        },
        "login": "LOGIN_NAME",
        "comment_password": "optional. Required if Vault integration is not used",
        "password": "PASSWORD",
        "comment_port": "optional, if we have to use any port other than the default port then we have to use this option",
        "port": "1433/3306/1521/5432",
        "comment_jdbcoptions": "Optional, this json object accepts all jdbc configuration options which are related to jdbc datastores",
        "jdbcoptions": {
          "fetchsize": "72000"
        },
        "comment_sslenabled": "optional, this option is currently supported for postgres",
        "sslenabled": false,
        "comment_primarykey": " The fields primarykey, lowerbound, upperbound, numPartitions are optional. Setting these fields allows Data Pull to read the rdbms source table/data as parallel partitions. This approach allows smaller table scans but using more connections. Please go through the following link to use these fields : https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases",
        "primarykey": "ID",
        "lowerBound": "1",
        "upperBound": "100",
        "numPartitions": "10",
        "comment_iswindowsauthenticated": "Optional, this option is to be used when the users wants to use windows authentication instead of sql authentication",
        "iswindowsauthenticated": "false",
        "comment_domain": "Optional, the domain group of the org you wants to connect when using windows authentication",
        "domain": "null",
        "comment_typeforteradata": "Optional, defaults to DEFAULT. Applies only to Teradata data stores. The value of this attribute is sent to the TYPE parameter in the JDBC connection string. For more information, please refer to https://teradata-docs.s3.amazonaws.com/doc/connectivity/jdbc/reference/current/jdbcug_chapter_2.html#BABJIHBJ",
        "typeforteradata": "DEFAULT"
      },
      "destination_elastic": {
        "platform": "elastic",
        "clustername": "IP_ADDR_OR_DNS_NAME",
        "port": "9201",
        "index": "Name of Index, equivalent of database",
        "comment_savemode": "Optional, the default value is upsert and please check here more information on this 'https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#_operation': ",
        "savemode": "upsert",
        "mappingid": "Optional:Any unique field of your data",
        "type": "type of index,equivalent of table.",
        "version": "Version of your ES",
        "login": "LOGIN_NAME",
        "password": "PASSWORD",
        "comment_post_migrate_command": "This is to execute shell commands ie mainly curl commands for elastic search",
        "post_migrate_commands": [
          {
            "shell": "curl -XDELETE http://IP_ADDR_OR_DNS_NAME:PORT/INDEX_NAME"
          }
        ],
        "comment_pre_migrate_command": "This is to execute shell commands ie mainly curl commands for elastic search",
        "pre_migrate_commands": [
          {
            "shell": "curl -XDELETE http://IP_ADDR_OR_DNS_NAME:PORT/INDEX_NAME"
          }
        ]
      },
      "source_kafka": {
        "platform": "kafka",
        "comment_bootstrapservers": "comma-separated list of bootstrap servers for the kafka cluster",
        "bootstrapservers": "KAFKA_BOOTSTRAP_SERVER:9092",
        "comment_schemaregistries": "comma-separated list of schema registry servers for the kafka cluster",
        "schemaregistries": "http://KAFKA_SCHEMA_REGISTRY:8081",
        "topic": "TOPIC_NAME",
        "comment_valuesubjectnamingstrategy": "Optional, defaults to TopicNameStrategy. Other options are RecordNameStrategy, TopicRecordNameStrategy. Subject naming strategy to use when getting/setting value's schema subject. Ref https://docs.confluent.io/6.0.0/schema-registry/serdes-develop/index.html#subject-name-strategy",
        "valuesubjectnamingstrategy": "TopicNameStrategy",
        "comment_valuesubjectrecordnamespace": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Namespace to use. e.g. if topic = test123, valuesubjectnamingstrategy = TopicRecordNameStrategy, valuesubjectrecordnamespace = com.expediagroup.dataplatform, valuesubjectrecordname = PotentialRmdEntry; then subject name for the topic's value  = test123-com.expediagroup.dataplatform.PotentialRmdEntry",
        "valuesubjectrecordnamespace": "somenamespace",
        "comment_valuesubjectrecordname": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Name to use",
        "valuesubjectrecordname": "somename",
        "comment_valueschemaversion": "optional, version of the topic's value schema, if the topic already has a value schema. If this is not specified, the latest version of the valueschema is picked. If there are no value schemas registered for the topic in schema registry, DataPull will register the schema of the valuefield to Schema Registry",
        "valueschemaversion": "1",
        "comment_keysubjectnamingstrategy": "Optional, defaults to TopicNameStrategy. Other options are RecordNameStrategy, TopicRecordNameStrategy. Subject naming strategy to use when getting/setting key's schema subject. Ref https://docs.confluent.io/6.0.0/schema-registry/serdes-develop/index.html#subject-name-strategy",
        "keysubjectnamingstrategy": "TopicNameStrategy",
        "comment_keysubjectrecordnamespace": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Namespace to use. e.g. if topic = test123, keysubjectnamingstrategy = TopicRecordNameStrategy, keysubjectrecordnamespace = com.expediagroup.dataplatform, keysubjectrecordname = PotentialRmdEntry; then subject name for the topic's key  = test123-com.expediagroup.dataplatform.PotentialRmdEntry",
        "keysubjectrecordnamespace": "somenamespace",
        "comment_keysubjectrecordname": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Name to use",
        "keysubjectrecordname": "somename",
        "comment_keyschemaversion": "optional, version of the topic's key schema, if the topic already has a key schema. If this is not specified, the latest version of the key schema is picked. If there are no key schemas registered for the topic in schema registry, DataPull will register the schema of the keyfield to Schema Registry",
        "keyschemaversion": "1",
        "comment_headerfield": "Optional. If set, this should be an Array[(String, Array[Byte])] to match Kafka's expectation of an array for headers, ref http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations . Please note this only works for DataPull running on Spark 3.0 or higher.",
        "headerfield": "some_array_field",
        "comment_keystorepath": "Optional. local path on spark cluster, to keystore jks file that is needed to authenticate DataPull as a producer client for secure Kafka brokers",
        "keystorepath": "/local/apath/to/jks/file",
        "comment_keystorepassword": "Optional. Password for optional keystore jks file",
        "keystorepassword": "passwrod_for_jks_file",
        "comment_truststorepath": "Optional. local path on spark cluster, to truststore jks file that is needed to authenticate DataPull as a producer client for secure Kafka brokers",
        "truststorepath": "/local/apath/to/jks/file",
        "comment_truststorepassword": "Optional. Password for optional truststore jks file",
        "truststorepassword": "passwrod_for_jks_file",
        "comment_keypassword": "Optional. Password for SSL key. Ref https://kafka.apache.org/25/javadoc/org/apache/kafka/common/config/SslConfigs.html#SSL_KEY_PASSWORD_CONFIG",
        "keypassword": "some_password",
        "comment_keyformat": "Optional, by default will be avro. Can also be string. If string is chosen, the key field should be of type string. If the field is a complex data type like a struct, you can use the to_json() spark sql command",
        "keyformat": "avro",
        "comment_valueformat": "Optional, by default will be avro. Can also be string. If string is chosen, the value field should be of type string. If the field is a complex data type like a struct, you can use the to_json() spark sql command",
        "valueformat": "avro",
        "comment_sparkoptions": "Optional, this is to enable users to use structured streaming-specific configurations. These options will supercede explicitely named options above. For example, setting topic in sparkoptions will supercede the value of the topic attribute above. All options can be found at the official spark documentation: http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations",
        "sparkoptions": {
          "comment_startingOffsets": "Optional, defaults to \"latest\" for streaming, \"earliest\" for batch. You can also use startingOffsetsByTimestamp which takes precendence over this. Ref http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations for more information",
          "startingOffsets": "\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"",
          "comment_endingOffsets": "Optional, defaults to \"latest\". You can also use endingOffsetsByTimestamp which takes precendence over this. Ref http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations for more information",
          "endingOffsets": "\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\""
        },
        "comment_schemaRegistries": "Deprecated. Please use schemaregistries attribute instead.",
        "schemaRegistries": "http://KAFKA_SCHEMA_REGISTRY:8081",
        "comment_offset": "Deprecated. Please use sparkoptions.startingOffsets instead",
        "offset": "earliest",
        "comment_deserializer": "Deprecated",
        "deSerializer": "org.apache.kafka.common.serialization.Deserializer",
        "comment_s3location": "Deprecated. we stage s3 data in a s3 location as we poll the topic many times. giving a location in the same env as the kafka topic is required and we append two UUID's(we have migration id and job id for each run, so we will appending these two UUID's at the end of the s3location to the user input)",
        "s3location": "s3a://BUCKET_NAME/FOLDER_PATH",
        "comment_groupid": "Deprecated. Optional, using this option will read the data where you are left off in the previous run with the same group id",
        "groupid": "KAFKA_GROUPID",
        "comment_requiredonlyvalue": "Deprecated. Optional, by default we will read both the key and value objects. if you want to get the only value then keep this option to true",
        "requiredonlyvalue": "false",
        "comment_payloadcolumnname": "Deprecated. this is the name of the payload column for the topic. for eg, value, body etc. default value for this option is `body` and this option only will have an effect when you have the option `requiredonlyvalue` to true",
        "payloadcolumnname": "body",
        "Comment_keyField": "Deprecated",
        "keyField": "some_keyfield",
        "Comment_valuefield": "Deprecated",
        "valuefield": "some_value",
        "comment_keyFormat": "Depcrecated, use keyformat attributed instead. Optional, you can set the serializer type for the key between string and avro, Default: string",
        "keyFormat": "string"
      },
      "destination_kafka": {
        "platform": "kafka",
        "comment_bootstrapservers": "comma-separated list of bootstrap servers for the kafka cluster",
        "bootstrapservers": "KAFKA_BOOTSTRAP_SERVER:9092",
        "comment_schemaregistries": "comma-separated list of schema registry servers for the kafka cluster",
        "schemaregistries": "http://KAFKA_SCHEMA_REGISTRY:8081",
        "topic": "TOPIC_NAME",
        "comment_valuefield": "Optional, defaulted to 'value'. Field in the dataset that should be used as the kafka topics value field. If not specified, DataPull will use the field named value in the resultset",
        "valuefield": "value",
        "comment_valuesubjectnamingstrategy": "Optional, defaults to TopicNameStrategy. Other options are RecordNameStrategy, TopicRecordNameStrategy. Subject naming strategy to use when getting/setting value's schema subject. Ref https://docs.confluent.io/6.0.0/schema-registry/serdes-develop/index.html#subject-name-strategy",
        "valuesubjectnamingstrategy": "TopicNameStrategy",
        "comment_valuesubjectrecordnamespace": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Namespace to use. e.g. if topic = test123, valuesubjectnamingstrategy = TopicRecordNameStrategy, valuesubjectrecordnamespace = com.expediagroup.dataplatform, valuesubjectrecordname = PotentialRmdEntry; then subject name for the topic's value  = test123-com.expediagroup.dataplatform.PotentialRmdEntry",
        "valuesubjectrecordnamespace": "somenamespace",
        "comment_valuesubjectrecordname": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Name to use",
        "valuesubjectrecordname": "somename",
        "comment_valueschemaversion": "optional, version of the topic's value schema, if the topic already has a value schema. If this is not specified, the latest version of the valueschema is picked. If there are no value schemas registered for the topic in schema registry, DataPull will register the schema of the valuefield to Schema Registry",
        "valueschemaversion": "1",
        "comment_keyfield": "optional, field in the dataset that should be used as the kafka topic's key field. If not set , the kafka topic key will not be set",
        "keyfield": "some_keyfield",
        "comment_keysubjectnamingstrategy": "Optional, defaults to TopicNameStrategy. Other options are RecordNameStrategy, TopicRecordNameStrategy. Subject naming strategy to use when getting/setting key's schema subject. Ref https://docs.confluent.io/6.0.0/schema-registry/serdes-develop/index.html#subject-name-strategy",
        "keysubjectnamingstrategy": "TopicNameStrategy",
        "comment_keysubjectrecordnamespace": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Namespace to use. e.g. if topic = test123, keysubjectnamingstrategy = TopicRecordNameStrategy, keysubjectrecordnamespace = com.expediagroup.dataplatform, keysubjectrecordname = PotentialRmdEntry; then subject name for the topic's key  = test123-com.expediagroup.dataplatform.PotentialRmdEntry",
        "keysubjectrecordnamespace": "somenamespace",
        "comment_keysubjectrecordname": "Optional, defaults to empty string. Used only with RecordNameStrategy, TopicRecordNameStrategy. Record Name to use",
        "keysubjectrecordname": "somename",
        "comment_keyschemaversion": "optional, version of the topic's key schema, if the topic already has a key schema. If this is not specified, the latest version of the key schema is picked. If there are no key schemas registered for the topic in schema registry, DataPull will register the schema of the keyfield to Schema Registry",
        "keyschemaversion": "1",
        "comment_headerfield": "Optional. If set, this should be an Array[(String, Array[Byte])] to match Kafka's expectation of an array for headers, ref http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations . Please note this only works for DataPull running on Spark 3.0 or higher.",
        "headerfield": "some_array_field",
        "comment_keystorepath": "Optional. local path on spark cluster, to keystore jks file that is needed to authenticate DataPull as a producer client for secure Kafka brokers",
        "keystorepath": "/local/apath/to/jks/file",
        "comment_keystorepassword": "Optional. Password for optional keystore jks file",
        "keystorepassword": "passwrod_for_jks_file",
        "comment_truststorepath": "Optional. local path on spark cluster, to truststore jks file that is needed to authenticate DataPull as a producer client for secure Kafka brokers",
        "truststorepath": "/local/apath/to/jks/file",
        "comment_truststorepassword": "Optional. Password for optional truststore jks file",
        "truststorepassword": "passwrod_for_jks_file",
        "comment_keypassword": "Optional. Password for SSL key. Ref https://kafka.apache.org/25/javadoc/org/apache/kafka/common/config/SslConfigs.html#SSL_KEY_PASSWORD_CONFIG",
        "keypassword": "some_password",
        "comment_sparkoptions": "Optional, this is to enable users to use structured streaming-specific configurations. These options will supercede explicitely named options above. For example, setting topic in sparkoptions will supercede the value of the topic attribute above. All options can be found at the official spark documentation: http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations",
        "sparkoptions": {},
        "comment_keyformat": "Optional, by default will be avro. Can also be string. If string is chosen, the key field should be of type string. If the field is a complex data type like a struct, you can use the to_json() spark sql command",
        "keyformat": "avro",
        "comment_valueformat": "Optional, by default will be avro. Can also be string. If string is chosen, the value field should be of type string. If the field is a complex data type like a struct, you can use the to_json() spark sql command",
        "valueformat": "avro"
      },
      "source_influxdb": {
        "platform": "influxdb",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "clustername": "localhost",
        "database": "mydb",
        "measurementname": "cpu",
        "login": "login",
        "password": "password",
        "vaultenv": "dev",
        "comment_vaultenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials"
      },
      "destination_influxDB": {
        "platform": "influxdb",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "comment_cluster": "This element accepts both IPs and Consul DNS names. Consul DNS names are required if Vault integration is used to fetch the credentials",
        "cluster": "IP_ADDR_OR_DNS_NAME",
        "database": "DATABASE_NAME",
        "comment_measurement": "destination measurement .This accept measurement field name/column .Only one column can be selected for measurements.If measurement is empty then provide the provide the measurment in the SQl query ",
        "measurement": "measurement field name",
        "comment_tags": "This is optional comma separated column name in destination table/measurement",
        "tags": [
          "tag1",
          "tag2"
        ],
        "comment_fields": "comma separated column name in destination table/measurement.At least one field is required",
        "fields": [
          "field1",
          "field2"
        ],
        "login": "LOGIN_NAME",
        "comment_password": "optional. Required if Vault integration is not used",
        "password": "PASSWORD",
        "comment_vaultenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "vaultenv": "dev"
      },
      "source_mongodb": {
        "platform": "mongodb",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "comment_cluster": "This element accepts both IPs and Consul DNS names. Consul DNS names are required if Vault integration is used to fetch the credentials. The port :27017 is appended to this field. Therefore, when providing multiple IP addresses, omit the port number from the last IP like so -> IP_ADDR:27017,IP_ADDR",
        "cluster": "IP_ADDR_OR_DNS_NAME",
        "comment_mapping": "authentication database is admin for ephemeral clusters; and the app database itself for dev/test/stage/prod clusters",
        "authenticationdatabase": "ADMIN_DATABASE_NAME",
        "comment_replicaset": "replicaset name of the cluster, specified for writes to mongodb",
        "replicaset": "REPLICASET_NAME",
        "database": "DATABASE_NAME",
        "collection": "COLLECTION_NAME",
        "login": "LOGIN_NAME",
        "comment_password": "optional. Required if Vault integration is not used",
        "password": "PASSWORD",
        "comment_overrideconnector": "Optional;Default value: false. use this flag to have DataPull read each MongoDB Document as a whole JSON into a field called jsonfield in the resultset. This is the recommended approach when reading MongoDB collections that have documents that do not conform to a single schema.",
        "overrideconnector": false,
        "comment_tmpfilelocation": "this is required when the oveerideconnector option is enabled to store the temp data while reading data from mongo. it can be a s3 location nor any random location in the spark nodes",
        "tmpfilelocation": "S3_LOCATION_OR_ANY_RANDOM_DIRECTORY",
        "comment_samplesize": "Optional, this is to set sampling of the mongo driver to fetch the schema for a collection.The default value for this 1000. this can be used if we have varying schemas but not varying data types for a single field",
        "samplesize": "1000",
        "comment_sparkoptions": "Optional, this is to enable users to use mongo specific configurations. all options can be found at the official spark mongo documentation: https://docs.mongodb.com/spark-connector/master/configuration/",
        "sparkoptions": {
          "replaceDocument": true,
          "ordered": false,
          "readPreference.name": "Primary"
        },
        "cpmment_pre_migrate_command": "Optional, this feauture will allow users to run pre migrate command for any operations on the collections",
        "pre_migrate_command": "{ delete: \"datapull_test\",deletes: [ { q: { user: \"abc123\" }, limit: 1 } ] }",
        "cpmment_post_migrate_commands": "Optional, this feauture will allow users to run pre migrate command for any operations on the collections",
        "post_migrate_commands": [
          {
            "query": "{ delete: \"datapull_test\",deletes: [ { q: { user: \"abc123\" }, limit: 1 } ] }"
          }
        ]
      },
      "destination_mongodb": {
        "platform": "mongodb",
        "comment_awsenv": "optional. Possible values are dev/test/stage/prod. Required if Vault integration is used to fetch the credentials",
        "awsenv": "dev",
        "comment_cluster": "This element accepts both IPs and Consul DNS names. Consul DNS names are required if Vault integration is used to fetch the credentials. The port :27017 is appended to this field. Therefore, when providing multiple IP addresses, omit the port number from the last IP like so -> IP_ADDR:27017,IP_ADDR",
        "cluster": "IP_ADDR_OR_DNS_NAME",
        "comment_mapping": "authentication database is admin for ephemeral clusters; and the app database itself for dev/test/stage/prod clusters",
        "authenticationdatabase": "ADMIN_DATABASE_NAME",
        "comment_replicaset": "replicaset name of the cluster, specified for writes to mongodb",
        "replicaset": "REPLICASET_NAME",
        "database": "DATABASE_NAME",
        "collection": "COLLECTION_NAME",
        "login": "LOGIN_NAME",
        "comment_password": "optional. Required if Vault integration is not used",
        "password": "PASSWORD",
        "comment_documentfromjsonfield": "Optional; default value is false. If set to true, the value of the field in the source dataset identified by the \"jsonfield\" attribute, is treated as a full json document and is written to Mongodb. All the remaining fields in the source dataset are ignored. This is useful for data sources like MSSQL that can produce JSON documents; this is also useful when moving sources like MongoDB, S3 etc that can have multiple schemas within a single collection/folder.",
        "documentfromjsonfield": false,
        "comment_jsonfield": "Optional, this is the column in which the whole document is stored as a json string and please note that if you are migrating it from mongo to mongo with multi schema(I.e when you use overrideconnector option) then we save the column as jsonfield so we need not to add this option to the json",
        "jsonfield": "jsonfield",
        "comment_replacedocuments": "Optional, this is true by default",
        "replacedocuments": true,
        "comment_sparkoptions": "Optional, this is to enable users to use mongo specific configurations. all options can be found at the official spark mongo documentation: https://docs.mongodb.com/spark-connector/master/configuration/",
        "sparkoptions": {
          "replaceDocument": true,
          "ordered": false,
          "readPreference.name": "Primary"
        },
        "comment_secret_store": "Optional, this should be used if the secret has to be retrieved from secretmanager. Accepatble value is aws_secrets_manager",
        "secret_store": "aws_secrets_manager",
        "comment_secret_name": "if the aws_secrets_manager as selected as secret_store then this field should be used to provide the secret name",
        "secret_name": "AWS_SECRET_NAME",
        "comment_secret_key_name": "Optional. The secret stored in Secrets Manager corresponding to a secret name, has a max size of 10 kb. If the secret is a JSON string (which is usually the case) or a binary map, then the secret_key_name specifies which JSON attribute/map key to use as the secret. If the secret_key_name is not specified then the whole string is used as the secret.",
        "secret_key_name": null,
        "cpmment_pre_migrate_command": "Optional, this feauture will allow users to run pre migrate command for any operations on the collections",
        "pre_migrate_command": "{ delete: \"datapull_test\",deletes: [ { q: { user: \"abc123\" }, limit: 1 } ] }",
        "cpmment_post_migrate_commands": "Optional, this feauture will allow users to run pre migrate command for any operations on the collections",
        "post_migrate_commands": [
          {
            "query": "{ delete: \"datapull_test\",deletes: [ { q: { user: \"abc123\" }, limit: 1 } ] }"
          }
        ]
      },
      "destination_cloudwatch": {
        "platform": "cloudwatch",
        "groupname": "cloud watch log group",
        "streamname": "cloud watch log stream name",
        "region": "cloud watch region",
        "comment_awsaccesskeyid": "Optional. If not provided the application will use service link role for accessing cloudwatch",
        "awsaccesskeyid": "AWS_ACCESS_KEY",
        "comment_awssecretaccesskey": "Optional. If not provided the application will use service link role for accessing cloudwatch",
        "awssecretaccesskey": "AWS_SECRET_ACCESS_KEY"
      },
      "destination_neo4j": {
        "platform": "neo4j",
        "cluster": "IP_ADDR_OR_DNS_NAME",
        "comment_node1": "Required. Information about the first (of possible two) classes of nodes to be upserted",
        "node1": {
          "label": "foodtype",
          "property_key": "foodtypeid",
          "properties_nonkey": [
            "foodtype"
          ],
          "comment_createormerge": "Optional, defaults to MERGE. Can either be CREATE or MERGE, and denotes whether Neo4j does a CREATE (insert, possibly leading to duplicates, but very fast) or a MERGE (upsert, no duplicates but slower) operation",
          "createormerge": "MERGE",
          "comment_createnodekeyconstraint": "Optional, defaults to true. If true, Data Pull creates a node key constraint for all the key properties of the node",
          "createnodekeyconstraint": true
        },
        "comment_node2": "Optional. Information about the second (of possible two) classes of nodes to be upserted.",
        "node2": {
          "label": "food",
          "property_key": "foodid",
          "properties_nonkey": [
            "food"
          ]
        },
        "comment_relation": "Optional. Attributes of the directed relation from node1 to node2. If this is ommitted, node1 and node2 will still be upserted",
        "relation": {
          "label": "relatedto",
          "comment_createormerge": "Optional, defaults to MERGE. Can either be CREATE or MERGE, and denotes whether Neo4j does a CREATE (insert, possibly leading to duplicates, but very fast) or a MERGE (upsert, no duplicates but slower) operation",
          "createormerge": "MERGE"
        },
        "login": "appuser",
        "password": "appuser"
      },
      "destination_snowflake": {
        "platform": "snowflake",
        "comment_url": "Specifies the hostname for your account in the following format: account_name.snowflakecomputing.com . However, note that your full account name might include additional segments that identify the region and cloud platform where your account is hosted.",
        "url": "youraccountnumber.awsregion.snowflakecomputing.com",
        "user": "snowflakeuser",
        "password": "password",
        "database": "snowflake database name",
        "schema": "schema in datbase",
        "comment_table": "Table to read/write. When used as a source platform, one can also specify a SQL query that evaluates to a view e.g (SELECT * FROM testtable1) A",
        "table": "testtable1",
        "comment_savemode": "Optional, This should be one of the Append/Overwrite/ErrorIfExists",
        "savemode": "Overwrite",
        "comment_options": "Specify configuation options (including session options) for the connector, ref https://docs.snowflake.com/en/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector . Note that these options will take precedence over url, user, etc. if they overlap. For example, if you specify sfSchema in this section, it will take precedence over schema in parent JSON object",
        "options": {
          "postactions": "DROP SCHEMA \"DEMO_DB\".\"TESTSCHEMA1\";"
        }
      },
      "destination_sftp": {
        "platform": "sftp",
        "host": "htp hostname/IP address",
        "path": "somepath/somefilename.json",
        "login": "ftpusername",
        "comment_password": "Optional; password for ftpusername. This attribute is optional/ignored if the pemfilepath attribute is specified.",
        "password": "ftppassword",
        "comment_pemfilepath": "Optional; local file path for the PEM RSA private key file for ftpusername. This attribute is not needed if the password attribute is specified. The password attribute is optional/ignored if the pemfilepath attribute is specified. When running on EMR, the jksfilepath should be specified as the S3 location of the PEM file in the format s3://<S3 bucket and folder path>/<PEM file name>, and the pemfilepath should be of the format /mnt/bootstrapfiles/<PEM file name>",
        "pemfilepath": "/mnt/bootstrapfiles/<PEM file name>",
        "comment_jksfilepath": "Optional; When running on EMR, the jksfilepath should be specified as the S3 location of the PEM file in the format s3://<S3 bucket and folder path>/<PEM file name>, and the pemfilepath should be of the format /mnt/bootstrapfiles/<PEM file name>",
        "jksfilepath": "s3://<S3 bucket and folder path>/<PEM file name>",
        "comment_fileformat": "Supported formats are parquet,json,csv,orc,avro. Tab-delimited files can be read by cboosing the fileformat as csv and setting the delimiter to \t",
        "fileformat": "json"
      },
      "destination_console": {
        "platform": "console",
        "comment_outputmode": "Optional, defaults to 'append'. Applies to streaming data only. Refer to \"Console Sink\" on https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html for all options",
        "outputmode": "append",
        "comment_numrows": "Optional, defaults to 20. Number of rows of the dataframe to show",
        "numrows": 20,
        "comment_truncate": "Optional, defaults to true. Controls whether to truncate long dataframe data",
        "truncate": true
      }
    }
  ],
  "cluster": {
    "comment_pipelinename": "This suggests the name of the pipeline for the migration job.",
    "pipelinename": "my_pipeline_name",
    "comment_awsenv": "The env in which you want to run the job in",
    "awsenv": "prod",
    "comment_terminateclusterafterexecution": "Optional, defaulted to true. If this is set to false, the cluster that is spun up by the pipeline stays alive after the data pull execution is complete; and is re-used for the next execution",
    "terminateclusterafterexecution": true,
    "comment_portfolio": "The portfolio which you belnogs to",
    "portfolio": "my_portfolio",
    "comment_product": "The product information for which this is being done",
    "product": "my_product",
    "comment_ec2instanceprofile": "The role with whioch the emr cluster(ec2-instances) has to be spun up so that they can access vault.",
    "ec2instanceprofile": "my-loader",
    "comment_componentinfo": "Dominion enforced UUID tag. ",
    "ComponentInfo": "00000000-0000-0000-0000-000000000000",
    "comment_cronexpression": "optional,this element will be used for scheduling the job and it will be defaulted for single run",
    "cronexpression": "21 * * * *",
    "comment_master_instance_type": "optional, Type of EC2 instance to use for the master node of the EMR cluster. Default value: m4.large",
    "master_instance_type": "m4.large",
    "comment_slave_instance_type": "optional, Type of EC2 instance to use for the slave node(s) of the EMR cluster. Default value: m4.large",
    "slave_instance_type": "m4.large",
    "comment_NodeCount": "optional; Number of nodes, the number of slave nodes will be NodeCount -1. Default: 6",
    "NodeCount": 6,
    "comment_sparksubmitparams": "optional, this option will give the ability to add the users to add the spark submit options for your use case",
    "sparksubmitparams": "--conf spark.driver.memory=10g --deploy-mode cluster --class DataMigrationFramework s3://BUCKET_NAME/FOLDER_PATH/DataMigrationFramework-1.0-SNAPSHOT-jar-with-dependencies_test.jar",
    "comment_emr_service_role": "optional, thsi option should be used set the emr service role. Default: emr_datapull_role",
    "emr_service_role": "emr_datapull_role"
  }
}